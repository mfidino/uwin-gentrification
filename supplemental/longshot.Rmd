---
title: "Supplementary Materials for"
output: pdf_document
date: ""
header-includes:
- \usepackage{titling}
- \pretitle{\begin{center}\LARGE\includegraphics[width=12cm]{science_logo.png}\\[\bigskipamount]}
- \posttitle{\end{center}}
- \setlength\parindent{24pt}
- \usepackage{indentfirst}
- \usepackage{caption}
- \DeclareCaptionLabelFormat{nospace}{#1#2}
- \captionsetup[figure]{labelformat=nospace}
- \captionsetup[table]{labelformat=nospace}
- \usepackage{bm}
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(bbplot)
library(viridis)
library(raster)
library(data.table)
library(sf)
library(knitr)
library(kableExtra)
source("../R/census_functions.R")
sf::sf_use_s2(FALSE)
```
\def\figurename{Figure S}
\def\tablename{Table S}
\captionsetup{labelfont={bf}}

\vspace{-1truemm}
\begin{center}
\textbf{\Large{Gentrification drives patterns of alpha and beta diversity in cities}}\\
\vspace{5truemm}
\large{Mason Fidino \textit{et al.}}\\
\vspace{5truemm}
Corresponding author: Mason Fidino, mfidino@lpzoo.org
\end{center}



## The PDF file includes:
\begin{itemize}
  \vspace{-0.2cm}\item[] Methods (Table S1)
  \vspace{-0.2cm}\item[] Additional gentrification metric details (Tables S2 to S9)
  \vspace{-0.2cm}\item[] Map of cities (Fig S1)
  \vspace{-0.2cm}\item[] City-specific alpha and beta diversity results (Fig S2 - SX)
  \vspace{-0.2cm}\item[] References (x - y)
\end{itemize}

\newpage

## Methods

### *Biological sampling*


We used data from 23 UWIN cities in the United States (U.S.)  for this study. Each city followed the same systematic study design, placing motion-triggered camera traps in urban greenspace along an urbanization gradient (see Magle et al. 2019 for details). Mammal data for this study came from 12 distinct sampling periods between 2019 and 2021. Camera deployments in each sampling period were about 35 days (sd = 13.01) and began on the first of January, April, July, or October of each year. Because UWIN cities joined the network at different times, the number of sampling periods among cities varied (median = 7, minimum = 2, maximum = 12). The median number of unique camera-trapping sites per city was 35 (minimum = 23, maximum = 104). 

Mammals in camera trap images were identified to species by trained experts. However, flying squirrel, gray squirrel, and cottontail rabbit species were summarized to either the subgenus or genus level given challenges in identifying them to the species level from camera trap images (Kays et al. 2022). For each camera deployment we counted the number of days each species was detected and the number of operational camera days, which were then used to estimate species occupancy and detectability within our multi-city multi-species occupancy model (Sutherland et al. 2016, Magle et al. 2021). 

Overall, 48 mammal species were photographed but our multi-city, multi-species occupancy model converged when limited to the 21 species that were most frequently detected (a minimum of 75 detection days across at least 3 cities). Gray squirrels (Sciurus carolinensis or Sciurus griseus) were detected most often (~41,300 detection days) while flying squirrels (Glaucomys sp.) were detected the least (79 detection days). See Table S1 for a summary of species detected across cities.

```{r table s1, echo = FALSE, message = FALSE, warning = FALSE}
dat <- data.table::fread(
  "../data/detection_data.csv",
  data.table = FALSE
)
dat <- dat[dat$Y>0,]
dat <- dat %>% 
  dplyr::group_by(City, Species) %>% 
  dplyr::summarise(
    nsite = length(unique(Site)),
    ndets = sum(Y)
  ) %>% 
  data.frame()
unq_places <- unique(dat$City)

pretty_names <- c(
  "Athens, GA",
  "Bay Area, CA",
  "Boston, MA",
  "Chicago, IL",
  "Denver, CO",
  "Houston, TX",
  "Indianapolis, IN",
  "Iowa City, IA",
  "Jackson, MS",
  "Little Rock, AR",
  "Madison, WI",
  "Metro LA, CA",
  "Washington D.C. metro area",
  "Phoenix, AZ",
  "Portland, Oregon",
  "Rochester, NY",
  "Sanford, FL",
  "Salt Lake City, UT",
  "Seattle, WA",
  "Saint Louis, MO",
  "Tacoma, WA",
  "Urbana, IL",
  "Wilmington, DE"
)
for(i in 1:length(unq_places)){
  dat$City[dat$City == unq_places[i]] <- pretty_names[i]
}

dat$Species <- gsub("_", " ", dat$Species)
dat <- dat[order(dat$City, dat$ndets),]

colnames(dat) <- c("City", "Species", "Sites detected", "Days detected")



dat %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l","l", "r","r"),
    caption = "The number of sites species were detected and number of detections per city. This table included all species, not only those we could use in our analysis. Table is ordered by city, and within each city species are sorted from fewest to most detections.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

```

### *Social-environmental variables*

We calculated two independent variables and included both in all models. First, to represent a gradient of urban intensity we calculated the percent impervious cover within 1 km of each site from the 2019 National Land Cover Database imperviousness dataset (Dewitz and U.S. Geological Survey, 2021). Second, we determined if each site was within 500 m of a gentrifying Census tract. To quantify gentrification across a wide range of cities we modified a two-step process described by Chapple et al. (2017). For the first step, we identified Census tracts that were vulnerable to gentrification in 2010 as tracts with at least 500 residents and two of these three qualities: 1) a median income less than the city’s average income, 2) a proportion of college-educated residents less than the city average, and 3) a proportion of nonwhite residents greater than the city average. To calculate gentrification vulnerability we used the 2010 US decennial Census data via the tidycensus package in R v 4.2.0 (Walker 2022, R Core Team 2022). For the second step, we used the 2019 American Community Survey (U.S. Census Bureau 2012) data to determine if a vulnerable Census tract became gentrified. Here, vulnerable tracts from the first step were identified as gentrified if they experienced a greater increase in median income between 2010 and 2019 than the average change across a city–after correcting for inflation–as well as one of two qualities: a change in college-educated residents or a change in the proportion of non-Hispanic white residents between 2010 and 2019 that exceeded the average change across the city. For additional details and summaries regarding this gentrification metric see the "Additional gentrification metric details" section of the supplemental material.

### *Associations between gentrification and our social-environmental variables*

Among cities, on average, 25% (sd = 11%) of camera sites were within 500m of a gentrified Census tract. At sites near gentrified Census tracts, 46% (sd = 20%) of land cover was impervious, on average, while sites not near gentrified Census tracts had an average of 25% (sd = 21%) impervious cover. Within cities, Urbana, Illinois had the lowest percent of sites within 500m of a gentrified Census tract (3%) and Phoenix, Arizona had the highest (50%). See Supplemental Material X1 for additional information on within-city variation in impervious cover and maps of gentrified and non-gentrified Census tracts.

With respect to the 2019 distribution of the variables we used to quantify gentrification across cities, the median per capita income of gentrified Census tracts (mean = \$68,785, sd = \$28,193) was roughly \$30,000 less than non-gentrified Census tracts (mean = \$98,678, sd = \$50,777). The proportion of non-Hispanic white residents living in gentrified Census tracts (mean = 0.28, sd = 0.26) was lower than non-gentrified Census tracts (mean = 0.48, sd = 0.30), and the proportion of people with a college degree in gentrified Census tracts (mean = 0.34, sd = 0.18) was slightly lower than non-gentrified Census tracts (mean = 0.48, sd = 0.23). Thus, gentrified Census Tracts still have lower incomes, fewer non-Hispanic white residents, and fewer college educated residents than non-gentrified Census Tracts. However, gentrified Census tracts saw greater than average shifts in these variables over time, such that the residents living there have become whiter, richer, and more educated. 

Gentrification may also be associated with an increase in urban greenspace. As such, we quantified whether gentrified Census tracts had a greater increase in the proportion of greenspace (i.e., developed, open space from NLCD data) over the same time frame we used to quantify gentrification (i.e., 2010 to 2019). We did not find this to be true. After averaging the proportional increase in urban greenspace across gentrified and not-gentrified census tracts in each city, the among-city range in both types of Census tracts was effectively zero (min = -0.01, max = 0.00). 

### *Brief overview of Statistical modeling*

We used a meta-analytic approach to quantify associations between gentrification and impervious cover and patterns of alpha and beta diversity across U.S. cities, using a Bayesian approach for all models (see following sections for more thorough explanation of our three statistical models). However, unlike more-common meta-analyses, which must contend with issues of publication bias that can distort results (Nakagawa et al. 2022), our analysis used all available UWIN data to parameterize both alpha- and beta diversity models, resulting in a more unbiased and data-driven evaluation of our hypothesis. 

To do so, we first fitted a Bayesian multi-city, multi-species occupancy model that included a first-order autoregressive term to account for repeat sampling across primary sampling periods within each city (Sutherland et al. 2016, Royle and Dorazio 2008, Magle et al. 2021). This model had three separate logit-linear predictors: one to indicate a species presence within a city’s species pool, one for site-level occupancy, and one for site-level detection probability. Following Magle et al. (2021), we included the distance of each city to the known margin of a species’ geographic range  in the first linear predictor, with positive and negative numbers respectively indicating cities within and outside a species range. Range data came from IUCN red list data (IUCN 2020). For site-level occupancy and detection, we included impervious cover, gentrification, and the interaction between the two as slope terms in the model. All species-level parameters shared information among species and cities via their random effect structure. Following a 1,000 step adaptation phase and a 125,000 step burn-in, we sampled the posterior 120,000 times across 4 chains. We thinned chains by 3 for a total of 40,000 posterior samples. We assessed model convergence through a visual inspection of traceplots and ensured Gelman-Rubin diagnostics were < 1.10 (Gelman et al., 2013). Following model convergence, we simulated species occupancy at each site across the entire study area from 5000 random samples of the occupancy model’s posterior distribution. 

For the alpha diversity model we calculated 1) the expected species richness at each site and 2) the standard deviation in this estimate across the 5,000 posterior samples. To limit the effect of individual years on these estimates we calculated species richness at a site across all possible sampling periods. This resulted in one estimate per site across cities. We then fitted a varying intercept, varying slope log-linear model to these data, which treated species richness as the response variable but also incorporated the associated uncertainty in this estimate (Kery and Royle citation). Intercept and slope terms were treated as city-level random effects. We included impervious cover, gentrification, and the interaction between the two as covariates. 

For the beta diversity model we calculated 1) pairwise community dissimilarity between pairs of sites within each city (i.e., Sørensen’s dissimilarity index) and 2) the standard deviation in this estimate across the 5,000 posterior samples (Legendre & Legendre 2012). Like the alpha diversity model, beta diversity estimates were made across all primary sampling periods. We then fitted a varying intercept, varying slope generalized dissimilarity model to these data, which treated pairwise dissimilarity as the response variable (GDM CITATION). This model used a clog link function and had an inverse link function of $1 - \text{exp}(-\mu)$, where $\mu$ is the linear predictor for one data point. Similar to the alpha diversity model, the beta diversity model incorporated the associated uncertainty in the beta diversity estimate. Intercepts and slopes were treated as city-level random effects. Because community composition may be more similar in nearby sites, we controlled for geographic distance between site pairs by including it as a covariate. We also included differences in impervious cover and gentrification between sites as covariates. However, because this model uses I-spline basis functions to incorporate possible non-linear responses we could not include an interaction between gentrification and impervious cover in this model (GDM CITATION). See the occupancy, alpha and beta diversity models section of the supplemental material.


### *The multi-city multi-species autologistic occupancy model*


This model is almost exactly the same model as we had used in Magle et al. (2022). For $s$ in $1,...,S$ species and $c$ in $1,...,C$ cities, $\pi_{sc}$ is the probability species $s$ is within city $c$. Futher, let $x_{sc}$ be a Bernoulli random variable that equals 1 if the species is within that city and is otherwise zero such that $x_{sc} \sim \text{Bernoulli}(\pi_{sc})$. We made $\pi_{sc}$ a function of one covariate--the distance a city is from the known edge of a species extent--using the logit link. This covariate was positive if a city was within a species extent and negative if it was outside. We compiled range information from IUCN red list data (IUCN, 2020). Thus the linear predictor for this level of the model was

$$
\text{logit}(\pi_{sc}) = \pmb{d}_s\pmb{h}_c
$$
where $\pmb{d}_s$ is a vector of species-specific covariates while $\pmb{h}_c$ is a vector of conformable regression coefficients where the leading element is 1 to accomodate the model intercept.

As the first level of the model estimates a species presence at the city-level, the next level estimates species presence within cities conditional on their presence in a city. Given that the number of sites and sampling periods varies across cities, we add a city subscript to define $i_c$ in $1,...,I_c$ sites  and $t_c$ in $1,...,T_c$ sampling periods. However, for simplicity we drop these specific subscripts while we explain the model. Additionally, let $z_{scit}$ be a Bernoulli random variable and $\psi_{scit}$ be the probability of occupancy. As such, $z_{scit} \sim \text{Bernoulli}(\psi_{scit}x_{sc})$. As with the previous level of the model, $\psi_{scit}$ can be made a function of covariates via the logit link. As a depature from the Magle et al. (2022) parameterization, we added a first-order autologistic term to account for any temporal dependence in occupancy status between adjacent sampling periods within a city. Thus, for the first time period in a city, the logit-linear predictor was

$$
\text{logit}(\psi_{scit=1}) = \pmb{\beta}_{sc}\pmb{f}_{ci}
$$
where $\pmb{\beta}_{sc}$ is a vector of species and city-specific parameters and $\pmb{f}_{ci}$ is a vector of conformable covariates whose first value is 1 for the model intercept. After the first sampling season, we added our autologistic term

$$
\text{logit}(\psi_{scit}) = \pmb{\beta}_{sc}\pmb{f}_{ci} + \theta_{sc}z_{scit-1} \text{, for t > 1.}
$$

For the data model, $y_{scit}$ was the number of days species $s$ was detected at city $c$ and site $i$ on sampling season $t$. Given $j_{cit}$ days of sampling, we assumed $y_{scit}$ is a binomial random variable conditional on species presence

$$
y_{scit} | z_{scit} \sim \text{Binomial}(j_{cit},\rho_{scit}z_{scit} )
$$

where $\rho_{scit}$ is the daily probability of detection that can be made a function of covariates with the logit link, 

$$
\text{logit}(\rho_{scit}) = \pmb{\alpha}_{sc}\pmb{g}_{ci}
$$
where $\pmb{\alpha}_{sc}$ is a vector of parameters and $\pmb{g}_{ci}$ is a vector of conformable covariates where the leading value is 1 to accommodate the intercept.

Given that some species occur across multiple cities, there are multiple levels in which species could partially share information. At the top-level of the model we have the simplest hierarchical parameterization for parameters associated to $\pi_{sc}$, namely that there is a community mean for each parameter of which species-level coefficients vary around. We show this for the model intercept with the understanding that the same parameterization applies to all logit-scale covariates in this part of the model.


\begin{equation}
\begin{aligned}
\bar{d}_{0s}   &\sim \text{Cauchy}(0, 2.5)\nonumber \\
\sigma_{d_0} &\sim \text{Inv-Gamma}(1,1)\nonumber \\
d_{0s}   &\sim \text{Normal}(\bar{d}_{0s}, \sigma_{d_0})\nonumber
\end{aligned}
\end{equation}

For the rest of the latent state model we add an additional hierarchical level to the model. However, this parameterization also aligns with the data model, and as such so we only describe it here once for the model intercept of the latent-state model. For example, for the model intercepts we begin with a community-level average among species and cities ($\bar{\beta}_0$). This parameter partially informs a species-level average among cities ($\bar{\beta}_{0s}$), which then informs species-specific coefficients in all cities ($\beta_{0sc}$). 

\begin{equation}
\begin{aligned}
\bar{\beta}_0   &\sim \text{Cauchy}(0, 2.5)\nonumber \\
\sigma_{\beta_0} &\sim \text{Inv-Gamma}(1,1)\nonumber \\
\bar{\beta}_{0s} &\sim \text{Normal}(\bar{\beta}_0, \sigma_{\beta_0}) \\
a_{\beta_0} &\sim \text{Uniform}(0,10) \\
b_{\beta_0} &\sim \text{Uniform}(0,10) \\
\sigma_{\beta_{0s}} &\sim \text{Inv-Gamma}(a_{\beta_0}, b_{\beta_0}) \\
\beta_{0sc} &\sim \text{Normal}(\bar{\beta}_{0s},\sigma_{\beta_{0s}} )
\end{aligned}
\end{equation}

We added the hyperparameters for the shape ($a_{\beta_0}$) and rate ($b_{\beta_0}$) of the Inv-Gamma distribution to account for the fact that some cities may only have one sampling period of data. Note that the above parameterization also applies to the autologistic term of the model ($\theta_{sc}$).

Finally, the latent state and data model included one other set of parameters to account for variation in occupancy or detectability across sampling seasons. As we have already added hierarchical structure via the centered parameterization of the other model parameters, we usd a non-centered parameterization here for this level of variability. Again, we show this for the latent state, but with a swapping of subscripts this could readily be applied to the data model as well.

\begin{equation}
\begin{aligned}
a_{\psi} &\sim \text{Uniform}(0,10) \\
b_{\psi} &\sim \text{Uniform}(0,10) \\
\sigma_{\psi c} &\sim \text{Inv-Gamma}(a_{\psi}, b_{\psi}) \\
\beta_{sct} &\sim \text{Normal}(0, \sigma_{\psi c})
\end{aligned}
\end{equation}

With this parameterization, $\beta_{sct}$ is a difference term that represents the logit-scale difference in occupancy for species $s$ at city $c$ from their average $\beta_{0sc}$ (i.e., the model intercept). Again, like with the other parameters in this part of the model, we used hyperparameters for the Inv-Gamma distribution because not every city had more than one season of data.

### *The alpha diversity meta-analytic model*

From our occupancy model we created a posterior distribution for the latent state of each species at each site across all cities ($z_{scit}$). From this, we derived two quantities for this model:

1. The mean expected species richness at each site across all sampling periods ($r_{ci}$). To do so, we calculated the number of unique species detected in city $c$ and site $i$ across the $t$ sampling seasons and took the median across 5000 posterior samples.
2. The standard deviation of the first quantity across those 5000 posterior samples ($\sigma_{ci}$). This quantifies our level of uncertainty with the first estimate.

Following this, let $\pmb{\beta}_c$ be a vector of city-specific regression coefficients and $\pmb{x}_{ci}$ be a vector of city and site specific covariates where the leading element is 1 to account for the intercept. Within the linear predictor we also added an additional residual variation term, $\epsilon_{ci}$, which was given a $\sim \text{Inv-Gamma}(1,1)$ prior. Thus, the log-linear predictor was

$$
\text{log}(\mu_{sci}) = \pmb{\beta}_c\pmb{x_{ci}} + \epsilon_{ci}
$$
and following Kery and Royle (YEAR), we accounted for variability in the response variable with an additional level of the model

$$
r_{ci} \sim \text{Normal}(\mu_{sci}, \sigma_{ci})
$$
We treated the intercept and slope terms as city-level random effects. For example, the prior for the model intercept was

\begin{equation}
\begin{aligned}
\bar{\beta}_0 & \sim \text{Cauchy}(0, 2.5)\nonumber \\
\sigma_{\beta_0} & \sim \text{Inv-Gamma}(1,1) \\
\beta_{0c} & \sim \text{Normal}(\bar{\beta}_0,\sigma_{\beta_0}) \\
\end{aligned}
\end{equation}

and the same specification was also applied to the slope terms as well (though not described here).

### *The beta diversity meta-analytic model*

From our occupancy model, we created a posterior distribution for the latent state of each species at each site across all cities ($z_{scit}$). From this, we derived three quantities:

1. The mean pairwise Sorensen dissimilarity between pairs of sites within each city across all sampling periods ($v_{cik}$, where the subscript k denotes one of the sites in city $c$ that is not the $ith$ site). To do so, we calculated the number of unique species detected in city $c$ and site $i$ across the $t$ sampling seasons. Following this, we calculated the Sorensen dissimilarity metric among all pairs of sites within each city using `vegan` in `R` across 5000 posterior samples. Finally,  we took the median across all posterior samples for each site-pair
2. The standard deviation of the first quantity across those 5000 posterior samples ($\sigma_{cik}$). This quantifies our level of uncertainty with the first estimate.
3. The mean expected number of unique species between a site-pair ($w_{cik}$). 

To estimate pairwise dissimilarity as a function of covariates we modified a generalized dissimilarity model to account for parametetric uncertainity of the response variable $v_{cik}$ (GDM citation). To do so, GDMs estimate the relationship between dissimilarity and environmental or spatial differences between pairs of sites with a clog link (Mokany et al. 2022):

$$
d_{cik} = 1 - \text{exp}(-\eta_{cik})
$$
where $d_{cik}$ is the biological dissimilarity between sites $i$ and $k$ within city $c$ and $\eta_{cik}$ is the predicted ecological distance between (i.e., the linear predictor). Given $p$ in 1,..,$P$ covariates, the ecological distance between sites is

$$
\eta_{cik} = b_0 + \sum_{p=1}^{P}|f_p(x_{cip}) - f_p(x_{ckp})|
$$
where $b_0$ is the model intercept (i.e., the expected pairwise dissimilarity between sites with identical environments). Covariates are further transformed within GDMs which 1) uses I-spline basis functions (Ramsay, 1988) and 2) constrains slope terms to be non-negative. Doing so allows the effect of each covariate to non-linearly vary while also ensuring that beta diversity increases monotonically as sites are more different from one another (a core assumption of this model). More specifically, the I-spline basis function for predictor $p$ with 3 basis functions (the default used for GDMs) is

$$
fp(x_{cip}) = \sum_{j = 1}^3a_{cpj}I_{pj}(x_{cip})
$$
where $a_{cpj}$ is a non-negative coefficient for the $jth$ I-spline and $I_{pj}$ is the $jth$ I-spline of the covariate $x_{cip}$. For our own model, the binary gentrification status of site-pairs was not sent through an I-spline basis function. Instead, we used a dummy variable that took the value of 1 if a pair of sites differed in their gentrification status and was otherwise 0. Regardless, all slope terms were constrained to be non-negative. Thus, given $\eta_{cik}$ and the total number of species between site pairs ($w_{cik}$), the first level of our model is

\begin{equation}
\begin{aligned}
d_{cik} &= 1 - \text{exp}(-\eta_{cik}) \\
\sigma_{cik} &= \sqrt{\frac{d_{cik} - (1 - d_{cik})}{w_{cik}}} \\
\mu_{cik} &\sim \text{Half-Normal}(d_{cik}, \sigma_{cik})
\end{aligned}
\end{equation}

where $\sigma_{cik}$ is the binomial variance function (Mokany et al. 2022) and the half-Normal is constrained to be non-negative. Following this, we account for variation in the measurement of our response variable

$$
v_{cik} \sim \text{Half-Normal}(\mu_{cik}, \sigma_{cik})
$$
where again $\sigma_{cik}$ is measured based on the output of the occupancy model and provided as data to this model.

We treated all coefficients in the linear predictor as city-level random effects. For example for the model intercept the prior specification would be  $b_c \sim \text{Half-Normal}(\bar{b}, \sigma_b)$ where $b \sim \text{Half-Normal}(0, 10)$ and $sigma_b \sim \text{Inv-Gamma}(1,1)$. The Half-Normal distributions ensure that the coefficients will be non-negative.

## Additional gentrification metrics

As previously stated we used a slightly modified version of the gentrification metric proposed by Chapple et al. (2017). This metric used a two-step process to identify gentrification. First, for an area to be gentrifying, it must be vulnerable to gentrification at the start of the study (in this case that is 2010). For our version of this metric a census tract to be vulnerable to gentrification it must:

1. Have had at least 500 residents in year 2010.
2. Have had at least two of these three qualities 
  a. The median income of residents in the census tract must be lower than the city's average income.
  b. The proportion of college-educated residents in the census-tract must be lower than the proportion of college-educated residents across the city.
  c. The proportion of nonwhite residents in the census tract must be greater than the proportion of nonwhite residents across the city.
  

After calculating census tract vulnerability, we created a 500m radius buffer around each camera trapping location within a city and intersected that buffer with the census tract level vulnerability index. This buffer was chosen to capture the general area around each site that easily fell within the home range of any species in our analysis. For example, a site could be right on the edge of a non-vulnerable census tract that abuts a vulnerable census tract and we would want to identify that location as 'vulnerable.'

Of 999 UWIN sites across 20 cities, 474 (47.4%) were considered vulnerable to gentrification.

Following this, a census tract was considered gentrifying if at the end of the study (in this case that is 2019) a census tract:

1. Was vulnerable to gentrification in 2010.
2. The change in median income was greater than the average change across the city, after correcting for inflation.
3. Had at least one of these two qualities.
  a. The change in the proportion of college educated residents was greater than average change across the city.
  b. The change in the proportion of non-hispanic white residents was greater than the average change across the city.
  

Of the 474 sites that were vulnerable to gentrification, about half of them of them were gentrifying (n = 264, 55.7%). However,there is a substantial amount of variation among cities. For example, Urbana, IL only had one of thirty five sites near a gentrifying census tract while Phoenix, AZ had exactly half of their 96 sites in gentrifying areas.

### *Retrieving the census data*

We used the `tidycensus` package in `R` to query census data from the year 2010 and 2019 (CITATION). The 2010 data came from the 10-year decennial census whereas the 2019 data came from the 5-year American Community Survey (ACS).

Across these years we compiled data the aforementioned variables for all census tracts that were within the general area of a UWIN transect. To do so we created a bounding box around the camera trap locations for each city, added a 500m buffer, and then cropped the census data.

### *Step 1. Determine which areas are vulnerable to gentrification*

After cropping the census tracts to an area around a cities sampled location, we calculated the regional median (i.e., median income across census tracts) and identified census tracts that fell below that number in 2010.


```{r med_inc, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}

# the buffer around which you want to explore whether a site
#  is nearby a gentrifying area.
my_buffer <- 500

coords <- read.csv(
    "../data/gentri_all_coordinates.csv"
)

# gentrification metrics data.frame

# now convert coords to spatial object
coords <- sf::st_as_sf(
  coords,
  coords = c("Long", "Lat"),
  crs = 4326
)

coords$City <- gsub("phaz2", "phaz", coords$City)

med <- readRDS("../data/census_data/med_income/med_income_2010.RDS")


med <- lapply(
  med,
  function(x){
    sf::st_transform(
      x,
      crs = 4326
    )
  }
)



city_result <- vector("list", length = length(med))

gvals <- rep(NA, length(med))

# # add the city to each census tract
unq_city <- unique(coords$City)
city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  cat(paste("\n",i,"\n"))
  pb <- txtProgressBar(max = length(unq_city))
  for(j in 1:length(unq_city)){
    setTxtProgressBar(pb, j)
    found_cities <- unlist(city_dict)
    if(unq_city[j] %in% found_cities & unq_city[j] != "naca"){
      next
    }

  tmp <- suppressMessages(
      sf::st_intersection(
      med[[i]],
      coords[coords$City == unq_city[j],]
    )
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# The national capital is three places combined, so we may
#  as well combine them and remap.
has_naca <- which(
  sapply(city_dict, function(x) any(x == "naca"))
)
naca_stuff <- bind_rows(med[c(has_naca)])
med <- med[-has_naca]
med <- append(
  med,
  list(naca_stuff)
)



# remap
city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  cat(paste("\n",i,"\n"))
  pb <- txtProgressBar(max = length(unq_city))
  for(j in 1:length(unq_city)){
    setTxtProgressBar(pb, j)
    found_cities <- unlist(city_dict)
    if(unq_city[j] %in% found_cities & unq_city[j] != "naca"){
      next
    }
  tmp <-suppressMessages(
    sf::st_intersection(
      med[[i]],
      coords[coords$City == unq_city[j],]
    )
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# also there was one element in the list with no overlapping
#  sites, remove it
to_go <- which(
  sapply(city_dict, is.null)
)

city_dict <- city_dict[-to_go]
med <- med[-to_go]
city_data <- vector(
  "list",
  length = length(unq_city)
)
names(city_data) <- unq_city
for(i in 1:length(med)){
  
  # get utms
  my_utms <- longlat_to_utm(
    c(
      mean(st_bbox(med[[i]])[c("xmin","xmax")]),
      mean(st_bbox(med[[i]])[c("ymin","ymax")])
    )
  )

  tmp <- coords[
    coords$City %in% city_dict[[i]] ,
  ]
  tmp <- sf::st_transform(
    tmp,
    crs = my_utms
  )
  tmp <- split(
    tmp,
    factor(tmp$City)
  )
  for(j in 1:length(tmp)){
  
  my_bbox <- st_bbox(
    tmp[[j]]
  ) + c(-500, -500, 500, 500) 
  
  area_cropped <- sf::st_crop(
    sf::st_transform(
      med[[i]],
      crs = my_utms
    ),
    my_bbox
  )
  area_cropped$City <- names(tmp)[j]
  my_city <- which(
    names(city_data) == names(tmp)[j]
  )
  city_data[names(tmp)[j]] <- list(
    sf::st_transform(
      area_cropped,
      4326
    )
  )
  }
}
# combine, and then split by city
allmed <- dplyr::bind_rows(city_data)

citymed <- split(
  allmed,
  factor(allmed$City)
)

# go through each city now and compile the income metric
income_result <- vector("list", length = length(citymed))
for(i in 1:length(citymed)){
  gvals[i] <- quantile(
    citymed[[i]]$estimate,
    probs = c(0.5),
    na.rm = TRUE
  )
  # figure out which tracts are less than the 50th quantile
  citymed[[i]]$income <- citymed[[i]]$estimate < gvals[i]
  
  # and then figure out which sites are
  income_result[[i]] <- sf::st_intersection(
    coords,
    citymed[[i]]
  )
}

# combine all city_results
income_result <- dplyr::bind_rows(income_result)

```
```{r income_table, echo = FALSE}


aa <- table(income_result$City, income_result$income)
aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites that were above (False) and below (True) the median income across census tracts in 2010. This was used to help determine if sites were vulnerable to gentrification.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

```

Following this, we calculated the median educational attainment in 2010 across census tracts, which was calculated similarly to income. We simplified the census data into two categories for educational attainment: those with a college degree and those without a college degree.

```{r education_vuln, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
if(!file.exists("../data/census_data/education/gent_vulnerability.rds")){
h1 <- readRDS("../data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("../data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  
  
  # store these values in a list
  edumed[[i]] <- tmp1
  # get lat/long to convert to utm, needed for rasterizing.

  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$variable
  )

  edumed[[i]]$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]] <- edumed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  edumed[[i]]$prop <- edumed[[i]]$value / edumed[[i]]$total
  edumed[[i]] <- edumed[[i]][edumed[[i]]$variable == 'college degree',]
  
  edumed[[i]]$edu_under <- edumed[[i]]$prop < 
    median(edumed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  edumed,
  "../data/census_data/education/gent_vulnerability.rds"
)
edu_list_vuln <- edumed
}else{
  edu_list_vuln <- readRDS("../data/census_data/education/gent_vulnerability.rds")
}



# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list_vuln))
for(i in 1:length(edu_result)){
  tmp <- edu_list_vuln[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```

```{r edu_vuln_table, echo = FALSE}

aa <- table(edu_result$City, edu_result$edu_under)
aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites that were above (False) and below (True) the median educational attainment (i.e., college degree) across census tracts in 2010. This was used to help determine if sites were vulnerable to gentrification.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

```

We then calculated whether Census tracts had at least 500 people in 2010.
The educational attainment data also had information on the number of people in each census tract, so we used that to create a binary metric for whether or not a census tract in 2010 had at least 500 people living in them. Of the 8133 census tracts across all cities, only 114 of them had fewer than 500 residents.
```{r mt500, echo = FALSE, include = FALSE}

npeople <- edu_list_vuln

for(i in 1:length(npeople)){
  npeople[[i]]$mt500 <- npeople[[i]]$total >= 500
}


```

The final metric we obtained to determine the vulnerability of Census tracts was whether the proportion of non-hispanic white people living there was lower than the regional median.


```{r nwprop, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
if(!file.exists("../data/census_data/race/gent_vulnerability.rds")){
nw <- readRDS("../data/census_data/race/race_2010.RDS")


nw <- lapply(
  nw,
  dplyr::ungroup
)
nw <- dplyr::bind_rows(nw)

# project to 4326
nw <- sf::st_transform(
  nw,
  4326
)


#  we can use that over again.
nwmed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
nw_list <- vector("list", length = length(citymed))
names(nw_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(nwmed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      nw,
      mmed
    )
  )
  # store these values in a list
  nwmed[[i]] <- tmp1
  
  # we need to rnwce this to people with and without 
  #  a college degree
  nwmed[[i]]$variable <- gsub(
    "Black|Asian|Latino",
    "Non-white",
    nwmed[[i]]$variable
  )


  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  nwmed[[i]] <- nwmed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  nwmed[[i]]$prop <- nwmed[[i]]$value / nwmed[[i]]$total
  nwmed[[i]] <- nwmed[[i]][nwmed[[i]]$variable == 'Non-white',]
  
  nwmed[[i]]$nw_over <- nwmed[[i]]$prop > 
    median(nwmed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  nwmed,
  "../data/census_data/race/gent_vulnerability.rds"
)
nw_list <- nwmed
}else{
  nw_list <- readRDS("../data/census_data/race/gent_vulnerability.rds")
}

# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
nw_result <- vector("list", length = length(nw_list))
for(i in 1:length(nw_result)){
  tmp <- nw_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  nw_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

nw_result <- dplyr::bind_rows(nw_result)

```


```{r nw_vuln_table, echo = FALSE}
aa <- table(nw_result$City, nw_result$nw_over)

aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites near Census tracts that had more (False) or less (True) than the regional median of non-hispanic white residents in 2010. This was used to help determine if sites were vulnerable to gentrification.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()
```

Finally, after combining these metrics we can arrive at the number of sites that were vulnerable to gentrification in 2010.

```{r vuln_combo, echo = FALSE}
# store site-specific info
vuln_list <- vector("list", length = length(citymed))
# store census tract data too
vuln_list_poly <- vuln_list
if(
  !file.exists(
    "../data/census_data/vuln_census_tracts.rds") |
  !file.exists(
    "../data/census_data/vuln_sites.rds"
  )
){
for(i in 1:length(citymed)){
  gent_result <- citymed[[i]][,c("City", "income")]
  
  gent_result <- sf::st_join(
    gent_result,
    edu_list_vuln[[i]][,"edu_under"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    npeople[[i]][,"mt500"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    nw_list[[i]][,"nw_over"],
    largest = TRUE
  )
  tmp_coords <- coords[
    coords$City == names(citymed)[i],
  ]
  # convert to UTMs
  gent_result <- sf::st_transform(
    gent_result,
    longlat_to_utm(gent_result)
  )
  gent_result$tmpsi <- 1:nrow(gent_result)
  gent_result <- gent_result %>% 
  dplyr::group_by(tmpsi) %>% 
  dplyr::mutate(
    vulnerable = 
      (
        sum(
          c(income, edu_under, nw_over),
          na.rm = TRUE
        ) > 1
      ) * mt500
  ) %>% 
    dplyr::ungroup()
  gent_result <- gent_result[,
      -which(colnames(gent_result) == "tmpsi")
  ]
  # keep this spatial data, we'll need it later
  vuln_list_poly[[i]] <- sf::st_transform(
    gent_result,
    4326
  )
  
  tmp_coords <- sf::st_transform(
    tmp_coords,
    crs = sf::st_crs(gent_result)
  )
  tmp_buff <- sf::st_buffer(
    tmp_coords,
    my_buffer
  )
  
  g_tmp <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) %>% 
    dplyr::group_by(
      Site
    ) %>% 
    dplyr::summarise(
      vulnerable = sum(vulnerable, na.rm = TRUE)>0
    ) %>% 
    sf::st_intersection(
      .,
      tmp_coords
    ) %>% 
    sf::st_transform(
      .,
      crs = 4326
    )
  
  vuln_list[[i]] <- g_tmp
  
}
  save.RDS(
    vuln_list,
    "./data/census_data/vuln_sites.rds"
  )
  saveRDS(
    vuln_list_poly,
    "../data/census_data/vuln_census_tracts.rds"
  )
}else{
  vuln_list <- readRDS(
    "../data/census_data/vuln_sites.rds"
  )
  vuln_list_poly <- readRDS(
    "../data/census_data/vuln_census_tracts.rds"
  )
}

gent_result <- dplyr::bind_rows(vuln_list)

aa <- table(gent_result$City, gent_result$vulnerable)

aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites that were (True) and were not (False) vulnerable to gentrification in 2010.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()


```

### *Step 2. Determine which vulnerable Census tracts gentrified*

This step here required us to compare Census tracts over time (i.e., between 2010 and 2019). However, Census tracts are not constant and are regularly redrawn. As a result, it can be difficult to make a 1 to 1 comparison of Census tracts over time. To address this we used areal interpolation. To do so we first rasterized the 2010 census data at a 500m resolution. Second, we joined the 2019 Census tracts to these data and took the spatial average (i.e., weighted unions by their area). For example, if Census tracts did not change over time, this would technique would provide a close to direct comparison (i.e., a 2019 census tract would mostly intersect the rasterized values of the same 2010 census tract). If a census tract did change, the 2019 census tract would take the spatial average over the 2010 census tracts it intersected with. 

After doing this we calculated the median change in income and educational attainment. We also calculated if the proportion of non-hispanic white residents in census tracts increased more than the regional median increase (i.e., more non-hispanic white people moved in than average).




```{r education, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
if(!file.exists("../data/census_data/education/gent_classification_r2.rds")){
h1 <- readRDS("../data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("../data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
edu_list <- vector("list", length = length(citymed))
names(edu_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  edumed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    edumed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  edumed[[i]] <- lapply(
    edumed[[i]],
    function(x) sf::st_transform(
      x,
      my_utm
    )
  )
  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$start$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$end$variable
  )
  edumed[[i]]$start$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$end$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]]$start <- edumed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  edumed[[i]]$start$prop <- edumed[[i]]$start$value / edumed[[i]]$start$total
  edumed[[i]]$start <- edumed[[i]]$start[
    edumed[[i]]$start$variable ==  "college degree",
    ]
  
  edumed[[i]]$end <- edumed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(value, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  edumed[[i]]$end$prop19 <- edumed[[i]]$end$value / edumed[[i]]$end$total
  edumed[[i]]$end <- edumed[[i]]$end[
    edumed[[i]]$end$variable ==  "college degree",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(edumed[[i]]$start)[1],
    xmx = st_bbox(edumed[[i]]$start)[3],
    ymn = st_bbox(edumed[[i]]$start)[2],
    ymx = st_bbox(edumed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    edumed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_education <- suppressWarnings(
    raster::extract(
      census_raster,
      edumed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  edu_list[[i]] <- edumed[[i]]$end
  edu_list[[i]]$prop10 <- start_education
}

saveRDS(
  edu_list,
  "../data/census_data/education/gent_classification_r2.rds"
)
}else{
  edu_list <- readRDS("../data/census_data/education/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(edu_list)){

  edu_list[[i]]$prop_edu <- edu_list[[i]]$prop19 - edu_list[[i]]$prop10
  # get the mean change
  
  edu_list[[i]]$edu_over <- edu_list[[i]]$prop_edu >
    median(edu_list[[i]]$prop_edu, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list))
for(i in 1:length(edu_result)){
  tmp <- edu_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords[coords$City == names(edu_list)[i],],
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```


Here is a table that shows how many sites are above (TRUE) and below (FALSE) the average change in in educational attainment.

```{r edu_table, echo = FALSE}

aa <- table(edu_result$City, edu_result$edu_over)

aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites where the increase in educational attainment (i.e., a college degree) between 2010 and 2019 was (True) and was not (False) greater than the city average.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

row.names(aa) <- NULL
```



```{r nw_prop_change, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
if(!file.exists("../data/census_data/race/gent_classification_r2.rds")){
h1 <- readRDS("../data/census_data/race/race_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("../data/census_data/race/race_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
racemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
race_list <- vector("list", length = length(citymed))
names(race_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(racemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  racemed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    racemed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  racemed[[i]] <- lapply(
    racemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # we need to rracece this to people with and without 
  #  a college degree
  racemed[[i]]$start$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$start$variable
  )
  racemed[[i]]$end$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$end$variable
  )
  #and then calcualte proportion
  #  with a college degree
  racemed[[i]]$start <- racemed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  racemed[[i]]$start$prop <- racemed[[i]]$start$value / racemed[[i]]$start$total
  racemed[[i]]$start <- racemed[[i]]$start[
    racemed[[i]]$start$variable ==  "White",
    ]
  
  racemed[[i]]$end <- racemed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(estimate, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  racemed[[i]]$end$prop19 <- racemed[[i]]$end$value / racemed[[i]]$end$total
  racemed[[i]]$end <- racemed[[i]]$end[
    racemed[[i]]$end$variable ==  "White",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(racemed[[i]]$start)[1],
    xmx = st_bbox(racemed[[i]]$start)[3],
    ymn = st_bbox(racemed[[i]]$start)[2],
    ymx = st_bbox(racemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    racemed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_race <- suppressWarnings(
    raster::extract(
      census_raster,
      racemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  race_list[[i]] <- racemed[[i]]$end
  race_list[[i]]$prop10 <- start_race
}

saveRDS(
  race_list,
  "../data/census_data/race/gent_classification_r2.rds"
)
}else{
  race_list <- readRDS("../data/census_data/race/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(race_list)){
  race_list[[i]]$prop_race <- race_list[[i]]$prop19 - race_list[[i]]$prop10
  # get the mean change
  
  race_list[[i]]$race_over <- race_list[[i]]$prop_race >
    median(race_list[[i]]$prop_race, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
race_result <- vector("list", length = length(race_list))
for(i in 1:length(race_result)){
  tmp <- race_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  race_result[[i]] <- sf::st_intersection(
    coords[coords$City == names(race_list)[i],],
    tmp
  )
}

race_result <- dplyr::bind_rows(race_result)


```

```{r race_table, echo = FALSE}
aa <- table(race_result$City, race_result$race_over)
aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites that where the change in the proportion of non-hispanic white residents was greater than the city average between 2010 and 2019.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()
```

For income, in addition to areal interpolation we adjusted for inflation. To do so we used the [U.S. Bureau of Labor Statistics inflation calculator](https://www.bls.gov/data/inflation_calculator.htm) to determine how much the price of \$1 has changed between January 2010 and January 2019 (it is \$1.17). Thus, we multiplied the dollar values of median housing prices in 2000 by `1.17` before comparing changes in housing prices.


```{r income_change, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}
if(!file.exists("../data/census_data/med_income/gent_classification_r2.rds")){
h1 <- readRDS("../data/census_data/med_income/med_income_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("../data/census_data/med_income/med_income_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
pricemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
price_list <- vector("list", length = length(citymed))
names(price_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(pricemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  pricemed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    pricemed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  pricemed[[i]] <- lapply(
    pricemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(pricemed[[i]]$start)[1],
    xmx = st_bbox(pricemed[[i]]$start)[3],
    ymn = st_bbox(pricemed[[i]]$start)[2],
    ymx = st_bbox(pricemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    pricemed[[i]]$start,
    my_rast,
    field = "estimate",
    fun = "mean",
    na.rm = TRUE
  )
  
  # Extract based on the 2019 census tracts, mu
  start_price <- suppressWarnings(
    raster::extract(
      census_raster,
      pricemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the price list and save it
  price_list[[i]] <- pricemed[[i]]$end
  # Add year to value19
  colnames(price_list[[i]]) <- gsub(
    "value",
    "value19",
    colnames(price_list[[i]])
  )
  # add price in 2000, and multiply by 1.17
  price_list[[i]]$value10 <- start_price * 1.17
}

saveRDS(
  price_list,
  "../data/census_data/med_income/gent_classification_r2.rds"
)
}else{
  price_list <- readRDS("../data/census_data/med_income/gent_classification_r2.rds")
}

# Now we need to calculate if the price of houses increased

for(i in 1:length(price_list)){
  price_list[[i]]$income_change <- price_list[[i]]$estimate - price_list[[i]]$value10
  price_list[[i]]$income_over <- price_list[[i]]$income_change > median(price_list[[i]]$income_change, na.rm = TRUE)
    
}
# and then we need to figure out which of our sites fall within these areas
price_result <- vector("list", length = length(price_list))
for(i in 1:length(price_result)){
  tmp <- price_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  price_result[[i]] <- sf::st_intersection(
    coords[coords$City == names(price_list)[i],],
    tmp
  )
}

price_result <- dplyr::bind_rows(price_result)

```


```{r price_table, echo = FALSE}
aa <- table(price_result$City, price_result$income_over)

aa <- data.frame(
  City = pretty_names,
  False = aa[,1],
  True = aa[,2]
)

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r"),
    caption = "The number of sites that do (True) and do not (False) reside in Census tracts where the median income increased more than the regional median.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

row.names(aa) <- pretty_names

```

After compiling these metrics we are able to combine them with our gentrification vulnerability values to determine which Census tracts gentrified. Finally, we determined which sites were within 500m of a gentrified Census tract.


```{r gent_combo, echo = FALSE, include = FALSE, message = FALSE, warning = FALSE}

gent_list <- vector("list", length = length(citymed))
gent_list_poly <- gent_list
gent_list_prop <- gent_list
if(
  !file.exists(
    "../data/census_data/gent_census_tracts.rds") |
  !file.exists(
    "../data/census_data/gent_sites.rds"
  )
){
for(i in 1:length(citymed)){
  gent_result <- vuln_list_poly[[i]][,c("City", "vulnerable")]
  
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      edu_list[[i]][,"edu_over"],
      4326
    ),
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      price_list[[i]][,"income_over"],
      4326
    ),
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      race_list[[i]][,"race_over"],
      4326
    ),
    largest = TRUE
  )
  tmp_coords <- coords[
    coords$City == names(citymed)[i],
  ]
  # convert to UTMs
  gent_result <- sf::st_transform(
    gent_result,
    longlat_to_utm(gent_result)
  )
  gent_result$tmpsi <- 1:nrow(gent_result)
  gent_result <- gent_result %>% 
  dplyr::group_by(tmpsi) %>% 
  dplyr::mutate(
    gentrifying = 
      (
        sum(
          c(edu_over, race_over),
          na.rm = TRUE
        ) >= 1
      ) * vulnerable * income_over
  ) %>% 
    dplyr::ungroup()
  gent_result <- gent_result[,
      -which(colnames(gent_result) == "tmpsi")
  ]
  
  tmp_coords <- sf::st_transform(
    tmp_coords,
    crs = sf::st_crs(gent_result)
  )
  tmp_buff <- sf::st_buffer(
    tmp_coords,
    my_buffer
  )
  
  gent_prop <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) 
  gent_prop <- split(
    gent_prop,
    factor(gent_prop$Site)
  )
  

  gent_prop <- lapply(
    gent_prop,
    function(x){
      if(sum(x$gentrifying, na.rm = TRUE)>0){
        to_return <- sum(
          sf::st_area(x[x$gentrifying == 1,])
        ) / sum(sf::st_area(x))
      } else {
        to_return <- 0
      }
      return(as.numeric(to_return))
    }
  )
  gent_prop <- utils::stack(gent_prop)
  colnames(gent_prop) <- c("prop_gent", "Site")
  gent_prop$City <- tmp_coords$City
  gent_list_prop[[i]] <- gent_prop
  
  
  g_tmp <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) %>% 
    dplyr::group_by(
      Site
    ) %>% 
    dplyr::summarise(
      gentrifying = sum(gentrifying, na.rm = TRUE)>0,
      vulnerable = sum(vulnerable, na.rm = TRUE)>0
    ) %>% 
    sf::st_intersection(
      .,
      tmp_coords
    ) %>% 
    sf::st_transform(
      .,
      crs = 4326
    )
  
  gent_list[[i]] <- g_tmp
  gent_list_poly[[i]] <- sf::st_transform(
    gent_result,
    4326
  )
  
}
  saveRDS(gent_list_prop, "../data/census_data/gent_sites_proportion.rds")
  saveRDS(gent_list, "../data/census_data/gent_sites.rds")
  saveRDS(gent_list_poly, "../data/census_data/gent_census_tracts.rds")
  
} else {
  gent_list  <- readRDS(
    "../data/census_data/gent_sites.rds"
  )
  gent_list_poly <- readRDS(
    "../data/census_data/gent_census_tracts.rds"
  )
}


gent_result <- dplyr::bind_rows(gent_list)


```


```{r gent_table, echo = FALSE}
aa <- table(gent_result$City, gent_result$vulnerable, gent_result$gentrifying, dnn = c("city", "vulnerable", "gentrifying"))
row.names(aa) <- pretty_names

to_save <- data.frame(gent_result)

# remove duplicates
to_save <- to_save[!duplicated(to_save[,c("Site","City")]),]

aa <- to_save

aa <- aa %>% 
  dplyr::group_by(City) %>% 
  dplyr::summarise(
    vulnNotGent = sum(vulnerable * !gentrifying),
    gent = sum(gentrifying),
    nvulngent = sum(!vulnerable * !gentrifying)
  )
aa$City <- pretty_names

colnames(aa) <- c("City",
                  "Vulnerable but did not gentrify",
                  "Gentrified",
                  "Not vulnerable so did not gentrify")

row.names(aa) <- NULL
aa %>% 
  kableExtra::kbl(
    format = "latex",
    align = c("l", "r","r","r"),
    caption = "The number of sites that were vulnerable to gentrification in 2010 but did not gentrify, the number of sites that did gentrify, and the number of sites that were not vulnerable so could not gentrify.",
    longtable = TRUE,
    booktabs = TRUE
  ) %>% 
  kableExtra::kable_styling()

```