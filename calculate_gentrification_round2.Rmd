---
title: "Calculating a gentrification metric across UWIN sites"
output: pdf_document
author: Mason Fidino
header-includes:
   - \usepackage{bm}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(pander)
library(dplyr)
library(bbplot)
library(plot3D)
library(viridis)
library(sf)
library(raster)
source("./R/census_functions.R")

```



## The metric I think we should use

So, I had to go back to the drawing board a few times in order
to generate a gentrification metric that could be applied
to the 20 different UWIN cities in this study. Initially,
we were going to hitch our wagon to the metrics provided in Freeman (2009). However, after collecting all the necessary census data and calculating these metrics it became apparent that the Freeman (2009) paper quantified city-level segregation, not gentrification. Instead, Freeman's 2009 paper actually used a gentrification metric they proposed in a 2005 paper of theirs to identify gentrifying areas. The general 'rules' tied to this metric are that a census tract must (Freeman 2005):

1. Be located in the inner city.

2. Have a median income less than the 40th percentile of the metropolitan area at the beginning of the intercentennial.

3. Has a percentage of housing built over the past 20 years that is below the 40th per centile for the metropolitan area.

4. Has a percentage increase in educational attainment that is greater than that of the metropolitan area.

5. Housing prices increased during the decade.


Unfortunately, when using these rules most UWIN sites within cities were not located in 'gentrifying' areas. As a result, this metric made gentrification so rare that it would be impossible to make comparisons in gentrifying vs non-gentrifying areas.

Because of this, I scoured through the literature again to find some other metrics that we could use to identify gentrifying areas within a city. From this, it became apparent rather quickly that there is a lot of disagreement on what 'rules' should be used to identify gentrification. This is good news, in some regards, as it gives us a little bit of wiggle room. After some searching, I decided to give a slightly modified version of the gentrification metric used by Chapple et al. (2017). This metric uses a two-step process to identify gentrifying areas. First, for an area to be gentrifying, it must be vulnerable to gentrification at the start of the study (in this case that is 2010). For a census tract to be vulnerable to gentrification it must:

1. Have at least 500 residents in year 1.
2. Have at least two of these three qualities.
  a. The median income of residents in the census tract must be lower than the city's average income.
  b. The proportion of college-educated residents in the census-tract must be lower than the proportion of college-educated residents across the city.
  c. The proportion of nonwhite residents in the census tract must be greater than the proportion of nonwhite residents across the city.
  

After calculating the census tract vulnerability, I created a 250m radius buffer around each camera trapping location within a city and intersected that buffer with the census tract level vulnerability index. This buffer was chosen because many sites were close to the edge of a given census tract and I wanted to capture the general area around each site. 

Of 956 UWIN sites across 20 cities, 410 (42.9%) were considered vulnerable to gentrification (see more below).

Following this, a census tract was considered gentrifying if at the end of the study (in this case that is 2019) if the census tract:

1. Was vulnerable to gentrification in 2010.
2. Had at least two of these three qualities.
  a. The change in the proportion of college educated residents was greater than average change across the city.
  b. The change in the proportion of non-hispanic white residents was greater than the average change across the city.
  c. The change in median income was greater than the average change across the city, after correcting for inflation.
  


Of the 410 sites that were vulnerable to gentrification, about half of them of them (n = 186) are gentrifying. 



## The data

The objective of this analysis is to determine how historical patterns of gentrification are associated to patterns of urban biodiversity. As such, we needed to compile census data from multiple years. To do so, I used the `tidycensus` package in `R` to query census data from the year 2000, 2010, 2015, and 2019. The 2000 data came from the 10-year decennial census whereas the remaining data come from the 5-year American Community Survey (ACS). The 10 year gap between 2000 and 2010 was because the 2005 5-year ACS data was not available. The 5-year ACS data was used because the 1-year ACS data did not contain estimates for smaller towns. 

Across all of these years I compiled data on race, income, number of housing units educational attainment (Table 1) for all census tracts that fell within counties that were sampled. For a given city that is sampled, we are making the assumption that the metropoliation area is "the counties that are sampled."


# Step 1. Figuring out which areas are vulnerable to gentrification

## Step 1.1 Household income in 2010 less than reginonal median.

To calculate this I took the median income at the 40th percentile for
each census tract that fell within a county that was sampled in a given
UWIN city. Following this, I determined which census tracts were less
then that specific value. I then intersected the site coordinates on those census tracts to determine if that specific site fell within a 
census tract that had a median income less than the 40th percentile of the studies area in 2000. Unlike other metrics we calculated here, we don't need to compare
census tracts across time. Instead, all I did was calculated the 40th income
percentile in 2000 and used that as the cutoff for the 2019 census tract levels.

```{r med_inc, echo = FALSE}
med <- readRDS("./data/census_data/med_income/med_income_2010.RDS")


med <- lapply(
  med,
  function(x){
    sf::st_transform(
      x,
      crs = 4326
    )
  }
)

coords <- read.csv(
    "./data/gentri_all_coordinates.csv"
)

# gentrification metrics data.frame

# now convert coords to spatial object
coords <- sf::st_as_sf(
  coords,
  coords = c("Long", "Lat"),
  crs = 4326
)

coords$City <- gsub("phaz2", "phaz", coords$City)

city_result <- vector("list", length = length(med))

gvals <- rep(NA, length(med))

# # add the city to each census tract
unq_city <- unique(coords$City)
city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  for(j in 1:length(unq_city)){
  tmp <- sf::st_intersection(
    med[[i]],
    coords[coords$City == unq_city[j],]
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# The national capital is three places combined, so we may
#  as well combine them and remap.
has_naca <- which(
  sapply(city_dict, function(x) any(x == "naca"))
)
naca_stuff <- bind_rows(med[c(has_naca)])
med <- med[-has_naca]
med <- append(
  med,
  list(naca_stuff)
)



# remap

city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  for(j in 1:length(unq_city)){
  tmp <- sf::st_intersection(
    med[[i]],
    coords[coords$City == unq_city[j],]
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# also there was one element in the list with no overlapping
#  sites, remove it
to_go <- which(
  sapply(city_dict, is.null)
)

city_dict <- city_dict[-to_go]
med <- med[-to_go]
city_data <- vector(
  "list",
  length = length(unq_city)
)
names(city_data) <- unq_city
for(i in 1:length(med)){
  
  # get utms
  my_utms <- longlat_to_utm(
    c(
      mean(st_bbox(med[[i]])[c("xmin","xmax")]),
      mean(st_bbox(med[[i]])[c("ymin","ymax")])
    )
  )

  tmp <- coords[
    coords$City %in% city_dict[[i]] ,
  ]
  tmp <- sf::st_transform(
    tmp,
    crs = my_utms
  )
  tmp <- split(
    tmp,
    factor(tmp$City)
  )
  for(j in 1:length(tmp)){
  
  my_bbox <- st_bbox(
    tmp[[j]]
  ) + c(-500, -500, 500, 500) 
  
  area_cropped <- sf::st_crop(
    sf::st_transform(
      med[[i]],
      crs = my_utms
    ),
    my_bbox
  )
  area_cropped$City <- names(tmp)[j]
  my_city <- which(
    names(city_data) == names(tmp)[j]
  )
  city_data[names(tmp)[j]] <- list(
    sf::st_transform(
      area_cropped,
      4326
    )
  )
  }
}
# combine, and then split by city
allmed <- dplyr::bind_rows(city_data)

citymed <- split(
  allmed,
  factor(allmed$City)
)

# go through each city now and compile the income metric
income_result <- vector("list", length = length(citymed))
for(i in 1:length(citymed)){
  gvals[i] <- quantile(
    citymed[[i]]$estimate,
    probs = c(0.5),
    na.rm = TRUE
  )
  # figure out which tracts are less than the 50th quantile
  citymed[[i]]$income <- citymed[[i]]$estimate < gvals[i]
  
  # and then figure out which sites are
  income_result[[i]] <- sf::st_intersection(
    coords,
    citymed[[i]]
  )
}

# combine all city_results
income_result <- dplyr::bind_rows(income_result)

```

Here is a table that shows how many sites are above (FALSE) and below (TRUE) the 40th percentile of median income. Some cities look to have  a pretty even split, which is nice, while other cities (e.g. Salt Lake City, **scut** has many sites above the median income value).

```{r income_table, echo = FALSE}

pander::pander(table(income_result$City, income_result$income),
               caption = "The number of sites below the median income in 2010 for each city.")

```


## Step 1.2. Educational attainment less than regional median



```{r education_vuln, echo = FALSE}
h1 <- readRDS("./data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
edu_list <- vector("list", length = length(citymed))
names(edu_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.
if(!file.exists("./data/census_data/education/gent_vulnerability.rds")){
  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  
  
  # store these values in a list
  edumed[[i]] <- tmp1
  # get lat/long to convert to utm, needed for rasterizing.

  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$variable
  )

  edumed[[i]]$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]] <- edumed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  edumed[[i]]$prop <- edumed[[i]]$value / edumed[[i]]$total
  edumed[[i]] <- edumed[[i]][edumed[[i]]$variable == 'college degree',]
  
  edumed[[i]]$edu_under <- edumed[[i]]$prop < 
    median(edumed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  edumed,
  "./data/census_data/education/gent_vulnerability.rds"
)
edu_list <- edumed
}else{
  edu_list <- readRDS("./data/census_data/education/gent_vulnerability.rds")
}



# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list))
for(i in 1:length(edu_result)){
  tmp <- edu_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```


Here is a table that shows how many sites are above (TRUE) and below (FALSE) the average change in in educational attainment.

```{r edu_vuln_table, echo = FALSE}

pander::pander(table(edu_result$City, edu_result$edu_under),
               caption = "The number of sites below the 
               median educational attainment 50th percentile of educational attainment (i.e,. college degree) between 2000 and 2019.")

```


## Step 1.3: The census tract must have at least 500 people.

```{r mt500, echo = FALSE}
# We can calculate this from the previous query

npeople <- edu_list

for(i in 1:length(npeople)){
  npeople[[i]]$mt500 <- npeople[[i]]$total >= 500
}

```

## Step 1.4: The proportion of non-white people in a census tract is greater than the city median.

```{r nwprop, echo = FALSE}
nw <- readRDS("./data/census_data/race/race_2010.RDS")


nw <- lapply(
  nw,
  dplyr::ungroup
)
nw <- dplyr::bind_rows(nw)

# project to 4326
nw <- sf::st_transform(
  nw,
  4326
)


#  we can use that over again.
nwmed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
nw_list <- vector("list", length = length(citymed))
names(nw_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.
if(!file.exists("./data/census_data/race/gent_vulnerability.rds")){
  for(i in 1:length(nwmed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      nw,
      mmed
    )
  )
  # store these values in a list
  nwmed[[i]] <- tmp1
  
  # we need to rnwce this to people with and without 
  #  a college degree
  nwmed[[i]]$variable <- gsub(
    "Black|Asian|Latino",
    "Non-white",
    nwmed[[i]]$variable
  )


  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  nwmed[[i]] <- nwmed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  nwmed[[i]]$prop <- nwmed[[i]]$value / nwmed[[i]]$total
  nwmed[[i]] <- nwmed[[i]][nwmed[[i]]$variable == 'Non-white',]
  
  nwmed[[i]]$nw_over <- nwmed[[i]]$prop > 
    median(nwmed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  nwmed,
  "./data/census_data/race/gent_vulnerability.rds"
)
nw_list <- nwmed
}else{
  nw_list <- readRDS("./data/census_data/race/gent_vulnerability.rds")
}

# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
nw_result <- vector("list", length = length(nw_list))
for(i in 1:length(nw_result)){
  tmp <- nw_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  nw_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

nw_result <- dplyr::bind_rows(nw_result)

```


```{r nw_vuln_table, echo = FALSE}

pander::pander(table(nw_result$City, nw_result$nw_over),
               caption = "The number of sites over with more non-white people than the city average.")

```

## Step 1.5: Combining the vulnerability metrics.

The site MUST have > 500 people and at least two of the other qualities
to be considered vulnerable to gentrification.


```{r vuln_combo, echo = FALSE}

vuln_list <- vector("list", length = length(citymed))
for(i in 1:length(citymed)){
  gent_result <- citymed[[i]][,c("City", "income")]
  
  gent_result <- sf::st_join(
    gent_result,
    edu_list[[i]][,"edu_under"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    npeople[[i]][,"mt500"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    nw_list[[i]][,"nw_over"],
    largest = TRUE
  )
  tmp_coords <- coords[
    coords$City == names(citymed)[i],
  ]
  # convert to UTMs
  gent_result <- sf::st_transform(
    gent_result,
    longlat_to_utm(gent_result)
  )
  gent_result$tmpsi <- 1:nrow(gent_result)
  gent_result <- gent_result %>% 
  dplyr::group_by(tmpsi) %>% 
  dplyr::mutate(
    vulnerable = 
      (
        sum(
          c(income, edu_under, nw_over),
          na.rm = TRUE
        ) > 1
      ) * mt500
  ) %>% 
    dplyr::ungroup()
  gent_result <- gent_result[,
      -which(colnames(gent_result) == "tmpsi")
  ]
  
  tmp_coords <- sf::st_transform(
    tmp_coords,
    crs = sf::st_crs(gent_result)
  )
  tmp_buff <- sf::st_buffer(
    tmp_coords,
    250
  )
  
  g_tmp <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) %>% 
    dplyr::group_by(
      Site
    ) %>% 
    dplyr::summarise(
      vulnerable = sum(vulnerable, na.rm = TRUE)>0
    ) %>% 
    sf::st_intersection(
      .,
      tmp_coords
    ) %>% 
    sf::st_transform(
      .,
      crs = 4326
    )
  
  vuln_list[[i]] <- g_tmp
  
}

gent_result <- dplyr::bind_rows(vuln_list)

pander::pander(
  table(gent_result$City, gent_result$vulnerable),
  caption = "The number of sites that reside in census tracts we identified as vulnerable to gentrification."
)


```


# Step 2. Determine if a vulnerable location has undergone gentrification.

For a site to be considered gentrifying it must:

1. Have been vulnerable to gentrification in 2010.
2. Have at least two of the following qualities:

a. An increase in the proportion of college educated people that is greater than the city average.
b. An increase in the proportion of white people greater than the city average.
c. An increase in median household income greater than the city average (absolute value).

We already have the first part calculated, so we just need to
do the rest.



## Step 2.1. Has a percentage increase in educational attainment that is greater than that of the metropolitan area.


```{r education, echo = FALSE}
h1 <- readRDS("./data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
edu_list <- vector("list", length = length(citymed))
names(edu_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.
if(!file.exists("./data/census_data/education/gent_classification_r2.rds")){
  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_intersects(
      mmed,
      h1
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_intersects(
      mmed,
      h2
    )
  )
  # store these values in a list
  edumed[[i]] <- list(
    start = h1[unique(unlist(tmp1)),],
    end = h2[unique(unlist(tmp2)),]
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    c(
      mean(st_bbox(edumed[[i]]$start)[c(1,3)]),
      mean(st_bbox(edumed[[i]]$start)[c(2,3)])
    )
  )
  # and convert the 2000 and 2019 data to utms for each city
  edumed[[i]] <- lapply(
    edumed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$start$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$end$variable
  )
  edumed[[i]]$start$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$end$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]]$start <- edumed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  edumed[[i]]$start$prop <- edumed[[i]]$start$value / edumed[[i]]$start$total
  edumed[[i]]$start <- edumed[[i]]$start[
    edumed[[i]]$start$variable ==  "college degree",
    ]
  
  edumed[[i]]$end <- edumed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(value, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  edumed[[i]]$end$prop19 <- edumed[[i]]$end$value / edumed[[i]]$end$total
  edumed[[i]]$end <- edumed[[i]]$end[
    edumed[[i]]$end$variable ==  "college degree",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(edumed[[i]]$start)[1],
    xmx = st_bbox(edumed[[i]]$start)[3],
    ymn = st_bbox(edumed[[i]]$start)[2],
    ymx = st_bbox(edumed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    edumed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_education <- suppressWarnings(
    raster::extract(
      census_raster,
      edumed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  edu_list[[i]] <- edumed[[i]]$end
  edu_list[[i]]$prop00 <- start_education
}

saveRDS(
  edu_list,
  "./data/census_data/education/gent_classification_r2.rds"
)
}else{
  edu_list <- readRDS("./data/census_data/education/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(edu_list)){
  edu_list[[i]]$prop_edu <- edu_list[[i]]$prop19 - edu_list[[i]]$prop00
  # get the mean change
  
  edu_list[[i]]$edu_over <- edu_list[[i]]$prop_edu >
    median(edu_list[[i]]$prop_edu, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list))
for(i in 1:length(edu_result)){
  tmp <- edu_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```


Here is a table that shows how many sites are above (TRUE) and below (FALSE) the average change in in educational attainment.

```{r edu_table, echo = FALSE}

pander::pander(table(edu_result$City, edu_result$edu_over),
               caption = "The number of sites where the increase in educational attainment (i.e., a college degree) between 2010 and 2019 was greater than the city average.")

```

## Step 2.2: The proportion of white people living in a census tract is greater than the city average.


```{r nw_prop_change, echo = FALSE}
h1 <- readRDS("./data/census_data/race/race_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/race/race_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
racemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
race_list <- vector("list", length = length(citymed))
names(race_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.
if(!file.exists("./data/census_data/race/gent_classification_r2.rds")){
  for(i in 1:length(racemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_intersects(
      mmed,
      h1
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_intersects(
      mmed,
      h2
    )
  )
  # store these values in a list
  racemed[[i]] <- list(
    start = h1[unique(unlist(tmp1)),],
    end = h2[unique(unlist(tmp2)),]
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    c(
      mean(st_bbox(racemed[[i]]$start)[c(1,3)]),
      mean(st_bbox(racemed[[i]]$start)[c(2,3)])
    )
  )
  # and convert the 2000 and 2019 data to utms for each city
  racemed[[i]] <- lapply(
    racemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # we need to rracece this to people with and without 
  #  a college degree
  racemed[[i]]$start$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$start$variable
  )
  racemed[[i]]$end$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$end$variable
  )
  #and then calcualte proportion
  #  with a college degree
  racemed[[i]]$start <- racemed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  racemed[[i]]$start$prop <- racemed[[i]]$start$value / racemed[[i]]$start$total
  racemed[[i]]$start <- racemed[[i]]$start[
    racemed[[i]]$start$variable ==  "White",
    ]
  
  racemed[[i]]$end <- racemed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(estimate, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  racemed[[i]]$end$prop19 <- racemed[[i]]$end$value / racemed[[i]]$end$total
  racemed[[i]]$end <- racemed[[i]]$end[
    racemed[[i]]$end$variable ==  "White",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(racemed[[i]]$start)[1],
    xmx = st_bbox(racemed[[i]]$start)[3],
    ymn = st_bbox(racemed[[i]]$start)[2],
    ymx = st_bbox(racemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    racemed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_race <- suppressWarnings(
    raster::extract(
      census_raster,
      racemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  race_list[[i]] <- racemed[[i]]$end
  race_list[[i]]$prop10 <- start_race
}

saveRDS(
  race_list,
  "./data/census_data/race/gent_classification_r2.rds"
)
}else{
  race_list <- readRDS("./data/census_data/race/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(race_list)){
  race_list[[i]]$prop_race <- race_list[[i]]$prop19 - race_list[[i]]$prop10
  # get the mean change
  
  race_list[[i]]$race_over <- race_list[[i]]$prop_race >
    median(race_list[[i]]$prop_race, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
race_result <- vector("list", length = length(race_list))
for(i in 1:length(race_result)){
  tmp <- race_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  race_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

race_result <- dplyr::bind_rows(race_result)


```

```{r race_table, echo = FALSE}

pander::pander(table(race_result$City, race_result$race_over),
               caption = "The number of sites where the change in the proportion of white people was greater than the city average.")

```


## Step 2.3: The change in median income was greater than the city average between 2010 and 2019.

Calculating this is similar to educational attainment
within a given census tract. However, we also need to account for inflation in these calculations. I went to the [U.S. Bureau of Labor Statistics website] (https://www.bls.gov/data/inflation_calculator.htm) and used their inflation calculator to determine how much the price of \$1 has changed between January 2010 and January 2019 (it is \$1.17). Thus, I multiplied the dollar values of median housing prices in 2000 by `1.17` before comparing changes in housing prices.


```{r income_longshot, echo = FALSE}
h1 <- readRDS("./data/census_data/med_income/med_income_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/med_income/med_income_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
pricemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
price_list <- vector("list", length = length(citymed))
names(price_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.
if(!file.exists("./data/census_data/med_income/gent_classification_r2.rds")){
  for(i in 1:length(pricemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_intersects(
      mmed,
      h1
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_intersects(
      mmed,
      h2
    )
  )
  # store these values in a list
  pricemed[[i]] <- list(
    start = h1[unique(unlist(tmp1)),],
    end = h2[unique(unlist(tmp2)),]
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    c(
      mean(st_bbox(pricemed[[i]]$start)[c(1,3)]),
      mean(st_bbox(pricemed[[i]]$start)[c(2,3)])
    )
  )
  # and convert the 2000 and 2019 data to utms for each city
  pricemed[[i]] <- lapply(
    pricemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(pricemed[[i]]$start)[1],
    xmx = st_bbox(pricemed[[i]]$start)[3],
    ymn = st_bbox(pricemed[[i]]$start)[2],
    ymx = st_bbox(pricemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    pricemed[[i]]$start,
    my_rast,
    field = "estimate",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts, mu
  start_price <- suppressWarnings(
    raster::extract(
      census_raster,
      pricemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the price list and save it
  price_list[[i]] <- pricemed[[i]]$end
  # Add year to value19
  colnames(price_list[[i]]) <- gsub(
    "value",
    "value19",
    colnames(price_list[[i]])
  )
  # add price in 2000, and multiply by 1.17
  price_list[[i]]$value10 <- start_price * 1.17
}

saveRDS(
  price_list,
  "./data/census_data/med_income/gent_classification_r2.rds"
)
}else{
  price_list <- readRDS("./data/census_data/med_income/gent_classification_r2.rds")
}

# Now we need to calculate if the price of houses increased

for(i in 1:length(price_list)){
  price_list[[i]]$income_change <- price_list[[i]]$estimate - price_list[[i]]$value10
  price_list[[i]]$income_over <- price_list[[i]]$income_change > median(price_list[[i]]$income_change, na.rm = TRUE)
    
}
# and then we need to figure out which of our sites fall within these areas
price_result <- vector("list", length = length(price_list))
for(i in 1:length(price_result)){
  tmp <- price_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  price_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

price_result <- dplyr::bind_rows(price_result)

```

Here is a table that shows how many sites reside in census tracts that have increased in price (TRUE) over time and those that have not (FALSE)

```{r price_table, echo = FALSE}

pander::pander(table(price_result$City, price_result$income_over),
               caption = "The number of sites that reside in census tracts whose housing prices have increased between 2000 and 2019.")

```

## Step 2.4: Combining the gentrification metrics.


```{r gent_combo, echo = FALSE}

gent_combo <- gent_result[,c("City", "Site", "vulnerable")]

gent_combo <- sf::st_join(
  gent_combo,
  edu_result[,"edu_over"]
)
gent_combo <- sf::st_join(
  gent_combo,
  race_result[,"race_over"]
)
gent_combo <- sf::st_join(
  gent_combo,
  price_result[,"income_over"]
)


gent_combo <- gent_combo %>% 
  dplyr::group_by(Site, City) %>% 
  dplyr::mutate(
    gentrifying = 
      (
        sum(
          c(edu_over, race_over, income_over),
          na.rm = TRUE
        ) > 1
      ) * vulnerable
  )

pander::pander(
  table(gent_combo$City, gent_combo$gentrifying),
  caption = "The number of sites that reside in census tracts we identified as vulnerable to gentrification."
)


```

## Plotting out the results across cities

I'm plotting out all the census tracts in counties that were sampled in each city in two ways:

1. The census tracts that were vulnerable to gentrification.
2. The census tracts that were identified as gentrifying.

```{r city_plots, echo = FALSE}

race_vuln <- readRDS(
  "./data/census_data/race/gent_vulnerability.rds"
)

inc_vuln <- citymed

edu_vuln <- readRDS(
  "./data/census_data/education/gent_vulnerability.rds"
)

# will need to calculate this within the loop
ppl_vuln <- edu_vuln

race_gent <- race_list

my_coords <- sf::st_transform(
  coords,
  crs = 4326
)


for(i in 1:length(citymed)){
  # Figure out all the vulnerable census tracts across a city
   cityname <- names(citymed)[i]
   
   all_vuln <- inc_vuln[[i]][,"income"]
   all_vuln <- sf::st_transform(
     all_vuln,
     crs = 4326
   )
   all_vuln <- sf::st_join(
     all_vuln,
     race_vuln[[i]][,"nw_over"],
     largest = TRUE
   )
   all_vuln <- sf::st_join(
     all_vuln,
     edu_vuln[[i]][,"edu_under"],
     largest = TRUE
   )
   ppl_vuln[[i]]$mt500 <- ppl_vuln[[i]]$total >= 500
   all_vuln <- sf::st_join(
     all_vuln,
     ppl_vuln[[i]][,"mt500"],
     largest = TRUE
   )
   all_vuln$site <- 1:nrow(all_vuln)
   all_vuln <- all_vuln %>% 
     group_by(site) %>% 
     mutate(
       vulnerable = (sum(
         income, nw_over, edu_under
       ) > 1) * mt500
     ) %>% 
     ungroup()
   
   all_vuln$vulnerable <- as.logical(
     all_vuln$vulnerable
   )
   
   all_gent <- race_list[[i]][,c("race_over")]
   all_gent <- sf::st_transform(
     all_gent,
     crs = 4326
   )
   
   all_gent <- sf::st_join(
     all_gent,
     all_vuln[,"vulnerable"],
     largest = TRUE
   )
   all_gent <- sf::st_join(
     all_gent,
     sf::st_transform(
       price_list[[i]][,"income_over"],
       crs = 4326
     ),
     largest = TRUE
   )
   all_gent <- sf::st_join(
     all_gent,
     sf::st_transform(
       edu_list[[i]][,"edu_over"],
       crs = 4326
     ),
    largest = TRUE
   )
   
   # calculate gentrification:
   all_gent$site <- 1:nrow(all_gent)
   
   all_gent <- all_gent %>% 
   group_by(site) %>% 
     mutate(
       gentrifying = (sum(
         c(income_over, edu_over, race_over),
         na.rm = TRUE
       ) > 1) * vulnerable
     )
     
   plot(
     all_gent["vulnerable"],
     main = paste0(
       cityname, ": vulnerable"
       ),
     reset = FALSE,
     pal = c("gray60", "gray 40")
    )
   
   points(
     st_coordinates(
       my_coords
     ),
     pch = 19
   )
   
    plot(
     all_gent["gentrifying"],
     main = paste0(
       cityname, ": gentrifying"
       ),
     reset = FALSE,
     pal = c("gray60", "gray 40")
    )
   
   points(
     st_coordinates(
       my_coords
     ),
     pch = 19
   )
  
   
}



```