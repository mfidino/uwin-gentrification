---
title: "Calculating a gentrification metric across UWIN sites"
output: pdf_document
author: Mason Fidino
header-includes:
   - \usepackage{bm}
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
library(pander)
library(dplyr)
library(bbplot)
library(viridis)
library(sf)
library(raster)
source("./R/census_functions.R")

```



# The metric I think we should use

So, I had to go back to the drawing board a few times in order
to generate a gentrification metric that could be applied
to the 20 different UWIN cities in this study. Initially,
we were going to hitch our wagon to the metrics provided in Freeman (2009). However, after collecting all the necessary census data and calculating these metrics it became apparent that the Freeman (2009) paper quantified city-level segregation, not gentrification. Instead, Freeman's 2009 paper actually used a gentrification metric they proposed in a 2005 paper of theirs to identify gentrifying areas, and then they explored the relationship between gentrification and segregation in their 2009 paper. The general 'rules' tied to the Freeman (2005) gentrification metric was that a census tract must:

1. Be located in the inner city.

2. Have a median income less than the 40th percentile of the metropolitan area at the beginning of the intercentennial.

3. Have a percentage of housing built over the past 20 years that is below the 40th per centile for the metropolitan area.

4. Have a percentage increase in educational attainment that is greater than that of the metropolitan area.

5. Housing prices increased over a decade.

Unfortunately, when using these rules most UWIN sites within cities were not located in 'gentrifying' areas. As a result, this metric made gentrification so rare that it would be impossible to make comparisons in gentrifying vs non-gentrifying areas.

Because of this, I scoured through the literature to find some other metrics that we could use to identify gentrifying areas within a city. From this, it became apparent rather quickly that there is a lot of variability on what 'rules' should be used to identify gentrification. This is good news, in some regards, as it gives us a little bit of wiggle room to make a metric we can use. However, there was some commonality. Overall, to identify gentrification most metrics in the literature compare some combination of:

1. Changes in income within census tracts over time. Gentrifying locations have observed an increase in income.
2. Changes in educational attainment over time. Gentrifying locations have observed an increase in people with a college deegree.
3. Changes in racial composition over time. Gentrifying areas have more non-hispanic white residents.

Based on these common themes, I decided to use a slightly modified version of the gentrification metric proposed by Chapple et al. (2017). This metric used a two-step process to identify gentrification. First, for an area to be gentrifying, it must be vulnerable to gentrification at the start of the study (in this case that is 2010). For a census tract to be vulnerable to gentrification it must:

1. Have at least 500 residents in year 1.
2. Have at least two of these three qualities (Chapple had a fourth metric here, which was related to rental units, that I removed).
  a. The median income of residents in the census tract must be lower than the city's average income.
  b. The proportion of college-educated residents in the census-tract must be lower than the proportion of college-educated residents across the city.
  c. The proportion of nonwhite residents in the census tract must be greater than the proportion of nonwhite residents across the city.
  

After calculating census tract vulnerability, I created a 500m radius buffer around each camera trapping location within a city and intersected that buffer with the census tract level vulnerability index. This buffer was chosen because I wanted to capture the general area around each site that easily fell within the home range of the species we'll be modeling. For example, a site could be right on the edge of a non-vulnerable census tract that abuts a vulnerable census tract and we would want to identify that location as 'vulnerable.'

Of 962 UWIN sites across 20 cities, 459 (47.7%) were considered vulnerable to gentrification.

Following this, a census tract was considered gentrifying if at the end of the study (in this case that is 2019) a census tract:

1. Was vulnerable to gentrification in 2010.
2. The change in median income was greater than the average change across the city, after correcting for inflation.
3. Had at least one of these two qualities (Chapple required both of these, I relaxed it down to either or).
  a. The change in the proportion of college educated residents was greater than average change across the city.
  b. The change in the proportion of non-hispanic white residents was greater than the average change across the city.
  

Of the 459 sites that were vulnerable to gentrification, about half of them of them were gentrifying (n = 251). However,there is a substantial amount of variation among cities. For example, Urbana only had one of thirty five sites near a gentrifying census tract while Phoenix had exactly half of their 96 sites in gentrifying areas.

Before I jump into the results a bit more, one thing I wanted to bring up here is how I calculated change over time across census datasets to quantify gentrification. Census tracts can (and often do) change every ten years, which makes it difficult to make a 1 to 1 comparison of census tracts over time. To address this, I rasterized the 2010 census data at a 500m resolution and then extracted the data with the 2019 census tracts. A 500m resolution was used because increasing the resolution had a negligible influence on the results I collected. This technique is a way to approximate areal interpolation. For example, if census tracts did not change over time, this would technique would provide a close to direct comparison (i.e., a 2019 census tract would mostly intersect the rasterized values of the same 2010 census tract). If a census tract did change, the 2019 census tract would take the spatial average over the 2010 census tracts it intersected with. As such, you can sort of this as performing a spatial Riemann sum.

The rest of this document has some summary tables for each level of the classification process (for those that are interested, otherwise just skip over them). However, at the end of the document I generated some plots for each city. Based on these, it appears we may need to correct some spatial clustering in our alpha and beta diversity analysis, given that lots of sites are that gentrifying are nearby one another. Please look over the plot for your city and let me know if it passes a 'smell test.'



# How I got the data

I used the `tidycensus` package in `R` to query census data from the year 2010 and 2019. The 2010 data came from the 10-year decennial census whereas the 2019 data came from the 5-year American Community Survey (ACS).

Across these years I compiled data the aforementioned variables for all census tracts that were within the general area of a UWIN transect. To figure that out, I created a bounding box around the camera trap locations for each city, added 500m to each edge to make it a little larger, and then cropped the census data to that bounding box. Because of this we are making the assumption that the metropolitan area is "the general area that a UWIN partner samples," which I think is appropriate given standard UWIN study designs. 


# Step 1. Figuring out which areas are vulnerable to gentrification

## Step 1.1 Household income in 2010 less than regional median.

After cropping the census tracts to an area around a cities sampled location, I calculated the regional median (i.e., median income across census tracts) and identified census tracts that fell below that number.

```{r med_inc, echo = FALSE}

# the buffer around which you want to explore whether a site
#  is nearby a gentrifying area.
my_buffer <- 500
med <- readRDS("./data/census_data/med_income/med_income_2010.RDS")


med <- lapply(
  med,
  function(x){
    sf::st_transform(
      x,
      crs = 4326
    )
  }
)

coords <- read.csv(
    "./data/gentri_all_coordinates.csv"
)

# gentrification metrics data.frame

# now convert coords to spatial object
coords <- sf::st_as_sf(
  coords,
  coords = c("Long", "Lat"),
  crs = 4326
)

coords$City <- gsub("phaz2", "phaz", coords$City)

city_result <- vector("list", length = length(med))

gvals <- rep(NA, length(med))

# # add the city to each census tract
unq_city <- unique(coords$City)
city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  for(j in 1:length(unq_city)){
  tmp <- suppressMessages(
      sf::st_intersection(
      med[[i]],
      coords[coords$City == unq_city[j],]
    )
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# The national capital is three places combined, so we may
#  as well combine them and remap.
has_naca <- which(
  sapply(city_dict, function(x) any(x == "naca"))
)
naca_stuff <- bind_rows(med[c(has_naca)])
med <- med[-has_naca]
med <- append(
  med,
  list(naca_stuff)
)



# remap
city_dict <- vector(
  "list",
  length = length(med)
)

# map each element of the list to a given city
for(i in 1:length(med)){
  for(j in 1:length(unq_city)){
  tmp <-suppressMessages(
    sf::st_intersection(
      med[[i]],
      coords[coords$City == unq_city[j],]
    )
  )
  if(nrow(tmp)> 0){
    if(length(city_dict[[i]]) > 0){
    city_dict[[i]] <- c(city_dict[[i]], unq_city[j])
    } else {
      city_dict[[i]] <- unq_city[j]
    }
  }
  }
}

# also there was one element in the list with no overlapping
#  sites, remove it
to_go <- which(
  sapply(city_dict, is.null)
)

city_dict <- city_dict[-to_go]
med <- med[-to_go]
city_data <- vector(
  "list",
  length = length(unq_city)
)
names(city_data) <- unq_city
for(i in 1:length(med)){
  
  # get utms
  my_utms <- longlat_to_utm(
    c(
      mean(st_bbox(med[[i]])[c("xmin","xmax")]),
      mean(st_bbox(med[[i]])[c("ymin","ymax")])
    )
  )

  tmp <- coords[
    coords$City %in% city_dict[[i]] ,
  ]
  tmp <- sf::st_transform(
    tmp,
    crs = my_utms
  )
  tmp <- split(
    tmp,
    factor(tmp$City)
  )
  for(j in 1:length(tmp)){
  
  my_bbox <- st_bbox(
    tmp[[j]]
  ) + c(-500, -500, 500, 500) 
  
  area_cropped <- sf::st_crop(
    sf::st_transform(
      med[[i]],
      crs = my_utms
    ),
    my_bbox
  )
  area_cropped$City <- names(tmp)[j]
  my_city <- which(
    names(city_data) == names(tmp)[j]
  )
  city_data[names(tmp)[j]] <- list(
    sf::st_transform(
      area_cropped,
      4326
    )
  )
  }
}
# combine, and then split by city
allmed <- dplyr::bind_rows(city_data)

citymed <- split(
  allmed,
  factor(allmed$City)
)

# go through each city now and compile the income metric
income_result <- vector("list", length = length(citymed))
for(i in 1:length(citymed)){
  gvals[i] <- quantile(
    citymed[[i]]$estimate,
    probs = c(0.5),
    na.rm = TRUE
  )
  # figure out which tracts are less than the 50th quantile
  citymed[[i]]$income <- citymed[[i]]$estimate < gvals[i]
  
  # and then figure out which sites are
  income_result[[i]] <- sf::st_intersection(
    coords,
    citymed[[i]]
  )
}

# combine all city_results
income_result <- dplyr::bind_rows(income_result)

```

Here is a table that shows how many sites are above (FALSE) and below (TRUE) the regional median income.

```{r income_table, echo = FALSE}


pretty_names <- c(
  "Athens, GA",
  "Bay Area, CA",
  "Boston, MA",
  "Chicago, IL",
  "Denver, CO",
  "Houston, TX",
  "Indianapolis, IN",
  "Iowa City, IA",
  "Jackson, MS",
  "Little Rock, AR",
  "Madison, WI",
  "Metro LA, CA",
  "National Capital",
  "Phoenix, AZ",
  "Portland, Oregon",
  "Rochester, NY",
  "Sanford, FL",
  "Salt Lake City, UT",
  "Seattle, WA",
  "Saint Louis, MO",
  "Tacoma, WA",
  "Urbana, IL",
  "Wilmington, DE"
)

aa <- table(income_result$City, income_result$income)
row.names(aa) <- pretty_names
pander::pander(aa,
               caption = "The number of sites below the median income in 2010 for each city.")

```


## Step 1.2. Educational attainment less than regional median in 2010.

This was calculated in the same way as the income data. I simplified the census data into two categories for educational attainment: those with a college degree and those without a college degree.

```{r education_vuln, echo = FALSE}
if(!file.exists("./data/census_data/education/gent_vulnerability.rds")){
h1 <- readRDS("./data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  
  
  # store these values in a list
  edumed[[i]] <- tmp1
  # get lat/long to convert to utm, needed for rasterizing.

  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$variable
  )

  edumed[[i]]$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]] <- edumed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  edumed[[i]]$prop <- edumed[[i]]$value / edumed[[i]]$total
  edumed[[i]] <- edumed[[i]][edumed[[i]]$variable == 'college degree',]
  
  edumed[[i]]$edu_under <- edumed[[i]]$prop < 
    median(edumed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  edumed,
  "./data/census_data/education/gent_vulnerability.rds"
)
edu_list_vuln <- edumed
}else{
  edu_list_vuln <- readRDS("./data/census_data/education/gent_vulnerability.rds")
}



# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list_vuln))
for(i in 1:length(edu_result)){
  tmp <- edu_list_vuln[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```


Here is a table that shows how many sites are below (TRUE) and above (FALSE) the regional median in educational attainment in 2010.

```{r edu_vuln_table, echo = FALSE}

aa <- table(edu_result$City, edu_result$edu_under)
row.names(aa) <- pretty_names

pander::pander(aa,
               caption = "The number of sites below the 
               median educational attainment 50th percentile of educational attainment (i.e,. college degree) between 2000 and 2019.")

```


## Step 1.3: The census tract must have at least 500 people.
 The education data from above also has information on the number of people in each census tract, so I just used that to create a binary metric for whether or not a census tract in 2010 had at least 500 people living in them (almost all of them did). As such, I'm not sharing a table here (e.g., of the 5661 census tracts across all cities, only 76 of them had fewer than 500 residents).
```{r mt500, echo = FALSE}

npeople <- edu_list_vuln

for(i in 1:length(npeople)){
  npeople[[i]]$mt500 <- npeople[[i]]$total >= 500
}


```

## Step 1.4: The proportion of non-white people in a census tract is greater than the city median.

Calculated same as above. I compiled the total number of people in a census tract as well as the number of non-hispanic white people living in a census tract to get this number (i.e., 1 - (number of non-Hispanic white people in a census tract / total number of residents in a census tract )).

```{r nwprop, echo = FALSE}
if(!file.exists("./data/census_data/race/gent_vulnerability.rds")){
nw <- readRDS("./data/census_data/race/race_2010.RDS")


nw <- lapply(
  nw,
  dplyr::ungroup
)
nw <- dplyr::bind_rows(nw)

# project to 4326
nw <- sf::st_transform(
  nw,
  4326
)


#  we can use that over again.
nwmed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
nw_list <- vector("list", length = length(citymed))
names(nw_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(nwmed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      nw,
      mmed
    )
  )
  # store these values in a list
  nwmed[[i]] <- tmp1
  
  # we need to rnwce this to people with and without 
  #  a college degree
  nwmed[[i]]$variable <- gsub(
    "Black|Asian|Latino",
    "Non-white",
    nwmed[[i]]$variable
  )


  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  nwmed[[i]] <- nwmed[[i]] %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  
  nwmed[[i]]$prop <- nwmed[[i]]$value / nwmed[[i]]$total
  nwmed[[i]] <- nwmed[[i]][nwmed[[i]]$variable == 'Non-white',]
  
  nwmed[[i]]$nw_over <- nwmed[[i]]$prop > 
    median(nwmed[[i]]$prop, na.rm = TRUE)

}

saveRDS(
  nwmed,
  "./data/census_data/race/gent_vulnerability.rds"
)
nw_list <- nwmed
}else{
  nw_list <- readRDS("./data/census_data/race/gent_vulnerability.rds")
}

# and then figure out which sites fall in these areas

# and then we need to figure out which of our sites fall within these areas
nw_result <- vector("list", length = length(nw_list))
for(i in 1:length(nw_result)){
  tmp <- nw_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  nw_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

nw_result <- dplyr::bind_rows(nw_result)

```


```{r nw_vuln_table, echo = FALSE}
aa <- table(nw_result$City, nw_result$nw_over)
row.names(aa) <- pretty_names

pander::pander(aa,
               caption = "The number of sites over with more non-white people than the city average.")

```

## Step 1.5: Combining the vulnerability metrics.

The site MUST have > 500 people and at least two of the other qualities to be considered vulnerable to gentrification. 


```{r vuln_combo, echo = FALSE}
# store site-specific info
vuln_list <- vector("list", length = length(citymed))
# store census tract data too
vuln_list_poly <- vuln_list
if(
  !file.exists(
    "./data/census_data/vuln_census_tracts.rds") |
  !file.exists(
    "./data/census_data/vuln_sites.rds"
  )
){
for(i in 1:length(citymed)){
  gent_result <- citymed[[i]][,c("City", "income")]
  
  gent_result <- sf::st_join(
    gent_result,
    edu_list_vuln[[i]][,"edu_under"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    npeople[[i]][,"mt500"],
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    nw_list[[i]][,"nw_over"],
    largest = TRUE
  )
  tmp_coords <- coords[
    coords$City == names(citymed)[i],
  ]
  # convert to UTMs
  gent_result <- sf::st_transform(
    gent_result,
    longlat_to_utm(gent_result)
  )
  gent_result$tmpsi <- 1:nrow(gent_result)
  gent_result <- gent_result %>% 
  dplyr::group_by(tmpsi) %>% 
  dplyr::mutate(
    vulnerable = 
      (
        sum(
          c(income, edu_under, nw_over),
          na.rm = TRUE
        ) > 1
      ) * mt500
  ) %>% 
    dplyr::ungroup()
  gent_result <- gent_result[,
      -which(colnames(gent_result) == "tmpsi")
  ]
  # keep this spatial data, we'll need it later
  vuln_list_poly[[i]] <- sf::st_transform(
    gent_result,
    4326
  )
  
  tmp_coords <- sf::st_transform(
    tmp_coords,
    crs = sf::st_crs(gent_result)
  )
  tmp_buff <- sf::st_buffer(
    tmp_coords,
    my_buffer
  )
  
  g_tmp <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) %>% 
    dplyr::group_by(
      Site
    ) %>% 
    dplyr::summarise(
      vulnerable = sum(vulnerable, na.rm = TRUE)>0
    ) %>% 
    sf::st_intersection(
      .,
      tmp_coords
    ) %>% 
    sf::st_transform(
      .,
      crs = 4326
    )
  
  vuln_list[[i]] <- g_tmp
  
}
  saveRDS(
    vuln_list,
    "./data/census_data/vuln_sites.rds"
  )
  saveRDS(
    vuln_list_poly,
    "./data/census_data/vuln_census_tracts.rds"
  )
}else{
  vuln_list <- readRDS(
    "./data/census_data/vuln_sites.rds"
  )
  vuln_list_poly <- readRDS(
    "./data/census_data/vuln_census_tracts.rds"
  )
}

gent_result <- dplyr::bind_rows(vuln_list)

aa <- table(gent_result$City, gent_result$vulnerable)
row.names(aa) <- pretty_names

pander::pander(
  aa,
  caption = "The number of sites that reside in census tracts we identified as vulnerable to gentrification."
)


```


# Step 2. Determine if a vulnerable location has undergone gentrification.

## Step 2.1. Has a percentage increase in educational attainment that is greater than that of the metropolitan area.



```{r education, echo = FALSE}
if(!file.exists("./data/census_data/education/gent_classification_r2.rds")){
h1 <- readRDS("./data/census_data/education/education_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/education/education_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
edumed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
edu_list <- vector("list", length = length(citymed))
names(edu_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(edumed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  edumed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    edumed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  edumed[[i]] <- lapply(
    edumed[[i]],
    function(x) sf::st_transform(
      x,
      my_utm
    )
  )
  
  # we need to reduce this to people with and without 
  #  a college degree
  edumed[[i]]$start$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "advanced college degree|college graduate",
    "college degree",
    edumed[[i]]$end$variable
  )
  edumed[[i]]$start$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$start$variable
  )
  edumed[[i]]$end$variable <- gsub(
    "hs diploma|no hs diploma|some college",
    "no college degree",
    edumed[[i]]$end$variable
  )
  # now group and sum a bit, and then calcualte proportion
  #  with a college degree
  edumed[[i]]$start <- edumed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_est),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  edumed[[i]]$start$prop <- edumed[[i]]$start$value / edumed[[i]]$start$total
  edumed[[i]]$start <- edumed[[i]]$start[
    edumed[[i]]$start$variable ==  "college degree",
    ]
  
  edumed[[i]]$end <- edumed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(value, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  edumed[[i]]$end$prop19 <- edumed[[i]]$end$value / edumed[[i]]$end$total
  edumed[[i]]$end <- edumed[[i]]$end[
    edumed[[i]]$end$variable ==  "college degree",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(edumed[[i]]$start)[1],
    xmx = st_bbox(edumed[[i]]$start)[3],
    ymn = st_bbox(edumed[[i]]$start)[2],
    ymx = st_bbox(edumed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    edumed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_education <- suppressWarnings(
    raster::extract(
      census_raster,
      edumed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  edu_list[[i]] <- edumed[[i]]$end
  edu_list[[i]]$prop10 <- start_education
}

saveRDS(
  edu_list,
  "./data/census_data/education/gent_classification_r2.rds"
)
}else{
  edu_list <- readRDS("./data/census_data/education/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(edu_list)){
  edu_list[[i]]$prop_edu <- edu_list[[i]]$prop19 - edu_list[[i]]$prop10
  # get the mean change
  
  edu_list[[i]]$edu_over <- edu_list[[i]]$prop_edu >
    median(edu_list[[i]]$prop_edu, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
edu_result <- vector("list", length = length(edu_list))
for(i in 1:length(edu_result)){
  tmp <- edu_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  edu_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

edu_result <- dplyr::bind_rows(edu_result)

```


Here is a table that shows how many sites are above (TRUE) and below (FALSE) the average change in in educational attainment.

```{r edu_table, echo = FALSE}

aa <- table(edu_result$City, edu_result$edu_over)
row.names(aa) <- pretty_names

pander::pander(aa,
               caption = "The number of sites where the increase in educational attainment (i.e., a college degree) between 2010 and 2019 was greater than the city average.")

```

## Step 2.2: The proportion of non-hispanic white people living in a census tract is greater than the city average.


```{r nw_prop_change, echo = FALSE}
if(!file.exists("./data/census_data/race/gent_classification_r2.rds")){
h1 <- readRDS("./data/census_data/race/race_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/race/race_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
racemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
race_list <- vector("list", length = length(citymed))
names(race_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(racemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  racemed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    racemed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  racemed[[i]] <- lapply(
    racemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # we need to rracece this to people with and without 
  #  a college degree
  racemed[[i]]$start$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$start$variable
  )
  racemed[[i]]$end$variable <- gsub(
    "Asian|Latino|Black",
    "Non-white",
    racemed[[i]]$end$variable
  )
  #and then calcualte proportion
  #  with a college degree
  racemed[[i]]$start <- racemed[[i]]$start %>% 
    dplyr::group_by(GEOID, variable) %>% 
    dplyr::summarise(
      total = unique(summary_value...Total),
      value = sum(value, na.rm = TRUE),
      .groups = "drop_last"
    ) %>% 
    dplyr::ungroup()
  racemed[[i]]$start$prop <- racemed[[i]]$start$value / racemed[[i]]$start$total
  racemed[[i]]$start <- racemed[[i]]$start[
    racemed[[i]]$start$variable ==  "White",
    ]
  
  racemed[[i]]$end <- racemed[[i]]$end %>% 
  dplyr::group_by(GEOID, variable) %>% 
  dplyr::summarise(
    total = unique(summary_est),
    value = sum(estimate, na.rm = TRUE),
    .groups = "drop_last"
  ) %>% 
  dplyr::ungroup()
  racemed[[i]]$end$prop19 <- racemed[[i]]$end$value / racemed[[i]]$end$total
  racemed[[i]]$end <- racemed[[i]]$end[
    racemed[[i]]$end$variable ==  "White",
    ]

  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(racemed[[i]]$start)[1],
    xmx = st_bbox(racemed[[i]]$start)[3],
    ymn = st_bbox(racemed[[i]]$start)[2],
    ymx = st_bbox(racemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    racemed[[i]]$start,
    my_rast,
    field = "prop",
    fun = "mean"
  )
  
  # Extract based on the 2019 census tracts
  start_race <- suppressWarnings(
    raster::extract(
      census_raster,
      racemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the change list and save it
  race_list[[i]] <- racemed[[i]]$end
  race_list[[i]]$prop10 <- start_race
}

saveRDS(
  race_list,
  "./data/census_data/race/gent_classification_r2.rds"
)
}else{
  race_list <- readRDS("./data/census_data/race/gent_classification_r2.rds")
}

# and then figure out which sites fall in these areas

for(i in 1:length(race_list)){
  race_list[[i]]$prop_race <- race_list[[i]]$prop19 - race_list[[i]]$prop10
  # get the mean change
  
  race_list[[i]]$race_over <- race_list[[i]]$prop_race >
    median(race_list[[i]]$prop_race, na.rm = TRUE)
}
# and then we need to figure out which of our sites fall within these areas
race_result <- vector("list", length = length(race_list))
for(i in 1:length(race_result)){
  tmp <- race_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  race_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

race_result <- dplyr::bind_rows(race_result)


```

```{r race_table, echo = FALSE}
aa <- table(race_result$City, race_result$race_over)
row.names(aa) <- pretty_names
pander::pander(aa,
               caption = "The number of sites where the change in the proportion of white people was greater than the city average.")

```


## Step 2.3: The change in median income was greater than the city average between 2010 and 2019.

Calculating this is similar to educational attainment
within a given census tract. However, we also need to account for inflation in these calculations. I went to the [U.S. Bureau of Labor Statistics website](https://www.bls.gov/data/inflation_calculator.htm) and used their inflation calculator to determine how much the price of \$1 has changed between January 2010 and January 2019 (it is \$1.17). Thus, I multiplied the dollar values of median housing prices in 2000 by `1.17` before comparing changes in housing prices.


```{r income_longshot, echo = FALSE}
if(!file.exists("./data/census_data/med_income/gent_classification_r2.rds")){
h1 <- readRDS("./data/census_data/med_income/med_income_2010.RDS")
h1 <- lapply(
  h1,
  dplyr::ungroup
)
h1 <- dplyr::bind_rows(h1)

# project to 4326
h1 <- sf::st_transform(
  h1,
  4326
)

h2 <- readRDS("./data/census_data/med_income/med_income_2019.RDS")
h2 <- lapply(
  h2,
  dplyr::ungroup
)
h2 <- dplyr::bind_rows(h2)

h2 <- st_transform(
  h2,
  4326
)

#  we can use that over again.
pricemed <- vector("list", length = length(citymed))

# store the suggested GEOIDs using the explanation above.
price_list <- vector("list", length = length(citymed))
names(price_list) <- names(citymed)

# This takes a while to run so I don't want to do it unless it's needed.
#  thus I saved it as an RDS file.

  for(i in 1:length(pricemed)){
   # reproject h1 to
   mmed <- st_transform(
     citymed[[i]],
     4326
   )
  
   # get what intersects with a given city in 2000
  tmp1 <- suppressMessages(
      sf::st_crop(
      h1,
      mmed
    )
  )
  # and in 2019
  tmp2 <- suppressMessages(
    sf::st_crop(
      h2,
      mmed
    )
  )
  # store these values in a list
  pricemed[[i]] <- list(
    start = tmp1,
    end = tmp2
  )
  # get lat/long to convert to utm, needed for rasterizing.
  my_utm <- longlat_to_utm(
    pricemed[[i]]$start
  )
  # and convert the 2000 and 2019 data to utms for each city
  pricemed[[i]] <- lapply(
    pricemed[[i]],
    function(x) sf::st_transform(x, my_utm)
  )
  
  # create a raster of the area with a 500 meter resolution.
  my_rast <- raster(
    resolution = 500,
    crs = st_crs(my_utm)$proj4string,
    xmn = st_bbox(pricemed[[i]]$start)[1],
    xmx = st_bbox(pricemed[[i]]$start)[3],
    ymn = st_bbox(pricemed[[i]]$start)[2],
    ymx = st_bbox(pricemed[[i]]$start)[4]
  )
  
  # and turn the 2000 census tract data into a raster
  census_raster <- rasterize(
    pricemed[[i]]$start,
    my_rast,
    field = "estimate",
    fun = "mean",
    na.rm = TRUE
  )
  
  # Extract based on the 2019 census tracts, mu
  start_price <- suppressWarnings(
    raster::extract(
      census_raster,
      pricemed[[i]]$end,
      buffer = 0,
      layer = 1,
      fun = mean,
      na.rm = TRUE
    )
  )[,1]
  # tack onto the price list and save it
  price_list[[i]] <- pricemed[[i]]$end
  # Add year to value19
  colnames(price_list[[i]]) <- gsub(
    "value",
    "value19",
    colnames(price_list[[i]])
  )
  # add price in 2000, and multiply by 1.17
  price_list[[i]]$value10 <- start_price * 1.17
}

saveRDS(
  price_list,
  "./data/census_data/med_income/gent_classification_r2.rds"
)
}else{
  price_list <- readRDS("./data/census_data/med_income/gent_classification_r2.rds")
}

# Now we need to calculate if the price of houses increased

for(i in 1:length(price_list)){
  price_list[[i]]$income_change <- price_list[[i]]$estimate - price_list[[i]]$value10
  price_list[[i]]$income_over <- price_list[[i]]$income_change > median(price_list[[i]]$income_change, na.rm = TRUE)
    
}
# and then we need to figure out which of our sites fall within these areas
price_result <- vector("list", length = length(price_list))
for(i in 1:length(price_result)){
  tmp <- price_list[[i]]
  tmp <- sf::st_transform(
    tmp,
    sf::st_crs(coords)
  )
  price_result[[i]] <- sf::st_intersection(
    coords,
    tmp
  )
}

price_result <- dplyr::bind_rows(price_result)

```

Here is a table that shows how many sites reside in census tracts that have increased in price (TRUE) over time and those that have not (FALSE)

```{r price_table, echo = FALSE}
aa <- table(price_result$City, price_result$income_over)
row.names(aa) <- pretty_names
pander::pander(aa,
               caption = "The number of sites that reside in census tracts where median income increased more than the regional median.")

```

## Step 2.4: Combining the gentrification metrics.


```{r gent_combo, echo = FALSE}

gent_list <- vector("list", length = length(citymed))
gent_list_poly <- gent_list
if(
  !file.exists(
    "./data/census_data/gent_census_tracts.rds") |
  !file.exists(
    "./data/census_data/gent_sites.rds"
  )
){
for(i in 1:length(citymed)){
  gent_result <- vuln_list_poly[[i]][,c("City", "vulnerable")]
  
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      edu_list[[i]][,"edu_over"],
      4326
    ),
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      price_list[[i]][,"income_over"],
      4326
    ),
    largest = TRUE
  )
  gent_result <- sf::st_join(
    gent_result,
    sf::st_transform(
      race_list[[i]][,"race_over"],
      4326
    ),
    largest = TRUE
  )
  tmp_coords <- coords[
    coords$City == names(citymed)[i],
  ]
  # convert to UTMs
  gent_result <- sf::st_transform(
    gent_result,
    longlat_to_utm(gent_result)
  )
  gent_result$tmpsi <- 1:nrow(gent_result)
  gent_result <- gent_result %>% 
  dplyr::group_by(tmpsi) %>% 
  dplyr::mutate(
    gentrifying = 
      (
        sum(
          c(edu_over, race_over),
          na.rm = TRUE
        ) >= 1
      ) * vulnerable * income_over
  ) %>% 
    dplyr::ungroup()
  gent_result <- gent_result[,
      -which(colnames(gent_result) == "tmpsi")
  ]
  
  tmp_coords <- sf::st_transform(
    tmp_coords,
    crs = sf::st_crs(gent_result)
  )
  tmp_buff <- sf::st_buffer(
    tmp_coords,
    my_buffer
  )
  
  g_tmp <- sf::st_intersection(
    gent_result,
    tmp_buff
  ) %>% 
    dplyr::group_by(
      Site
    ) %>% 
    dplyr::summarise(
      gentrifying = sum(gentrifying, na.rm = TRUE)>0
    ) %>% 
    sf::st_intersection(
      .,
      tmp_coords
    ) %>% 
    sf::st_transform(
      .,
      crs = 4326
    )
  
  gent_list[[i]] <- g_tmp
  gent_list_poly[[i]] <- sf::st_transform(
    gent_result,
    4326
  )
  
}
  saveRDS(gent_list, "./data/census_data/gent_sites.rds")
  saveRDS(gent_list_poly, "./data/census_data/gent_census_tracts.rds")
  
} else {
  gent_list  <- readRDS(
    "./data/census_data/gent_sites.rds"
  )
  gent_list_poly <- readRDS(
    "./data/census_data/gent_census_tracts.rds"
  )
}

gent_result <- dplyr::bind_rows(gent_list)

aa <- table(gent_result$City, gent_result$gentrifying)
row.names(aa) <- pretty_names


pander::pander(
  aa,
  caption = "The number of sites that reside in census tracts we identified as vulnerable to gentrification."
)


```

# Step 3. Plotting out the results across cities

Here is a plot for each city, it's pretty self-explanatory, though the legend does get overlaid on some cities a bit. Sorry!

```{r city_plots, echo = FALSE, out.width = "110%"}



for(i in 1:length(citymed)){
  # Figure out all the vulnerable census tracts across a city
     m <- matrix(c(1,1,2), ncol = 3, nrow = 1)
     layout(m)
   plot(
     gent_list_poly[[i]]["gentrifying"],
     main = paste0(
       pretty_names[i]
       ),
     reset = FALSE,
     pal = c("gray60", "gray40"),
     key.pos = NULL,
     mar = c(4,4,4,4)
    )

   my_bgs <- ifelse(
     gent_list[[i]]$gentrifying,
     "black",
     "white"
   )
   points(
     st_coordinates(
       gent_list[[i]]
     ),
     pch =21,
     cex = 1.1,
     bg = my_bgs,
   )
   par(mar = c(0,0,0,0))
   blank()
     
  
   legend(
     "center",
     c(
       "gentrifying census tract",
       "non-gentrifying census tract",
       "site < 500m of gentrifying census tract",
       "site > 500m of gentrifying census tract"
      ),
     pch = c(15,15,21,21),
     col = c("gray40", "gray60", "black", "black"),
     pt.bg = c(NA, NA, "black", "white"),
     bty = "n", pt.cex = 1.4,
     title = ""
    )
}



```


# References

Chapple, K., Waddell, P., Chatman, D., Zuk, M., Loukaitou-Sideris, A., Ong, P.,  Gorska, C. P., & Gonzalez, S. R. (2017). Developing a new methodology for analyzing potential displacement. Accessed at https://trid.trb.org/view/1464806

Freeman, L. (2005). Displacement or succession? Residential mobility in gentrifying neighborhoods. Urban Affairs Review, 40(4), 463-491.


Freeman, L. (2009). Neighbourhood diversity, metropolitan segregation and gentrification: What are the links in the US?. Urban Studies, 46(10), 2079-2101.